{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scheduler for AID2E Documentation","text":"<p>Welcome to the documentation for the Scheduler library, a Python package for scheduling and managing optimization trials for the ePIC EIC detector design using Ax.</p>"},{"location":"#overview","title":"Overview","text":"<p>The Scheduler library extends the Ax platform for Bayesian optimization with additional features for managing and executing trials on various compute backends. It's designed to facilitate parameter optimization for detector simulations and other computationally intensive tasks.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Ax Integration: Seamlessly works with the Ax platform for Bayesian optimization</li> <li>Multiple Job Types:</li> <li>Python functions</li> <li>Shell/Python scripts</li> <li>Containers (Docker/Singularity)</li> <li>Multiple Execution Backends:</li> <li>JobLib for local parallel execution</li> <li>Slurm for cluster computing</li> <li>PanDA for distributed computing</li> <li>Trial Management: Comprehensive trial state tracking and monitoring</li> <li>Flexible Execution: Support for synchronous or asynchronous execution modes</li> <li>Batch Processing: Submit multiple trials in parallel for efficient exploration</li> <li>Persistence: Save and load experiments to resume optimization</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation: How to install the Scheduler library</li> <li>Quick Start: Get up and running with simple examples</li> <li>Tutorials: Step-by-step guides for common use cases</li> <li>API Reference: Detailed documentation of classes and methods</li> </ul>"},{"location":"#about-this-documentation","title":"About This Documentation","text":"<p>This documentation is built using MkDocs with the Material theme, customized to have a GitBook-like appearance. It includes:</p> <ul> <li>Comprehensive API reference</li> <li>Step-by-step tutorials</li> <li>Architectural overview</li> <li>Code examples</li> <li>Dark/light mode toggle</li> </ul> <p>The documentation source is available in the GitHub repository under the <code>docs/</code> directory. Contributions to improve the documentation are welcome!</p>"},{"location":"#architecture","title":"Architecture","text":"<p>See the Architecture Overview for a high-level understanding of how the components interact.</p>"},{"location":"404/","title":"404 - Page Not Found","text":"<p>The page you are looking for does not exist or has been moved.</p> <p>Return to Home</p>"},{"location":"api_documentation/","title":"Automatic API Documentation","text":"<p>This project includes tools to automatically generate API documentation from your Python source code.</p>"},{"location":"api_documentation/#how-it-works","title":"How It Works","text":"<p>The automatic API documentation generator: 1. Scans your Python modules and classes 2. Extracts docstrings, function signatures, and type annotations 3. Generates formatted Markdown files that integrate with MkDocs 4. Creates a consistent API reference structure</p>"},{"location":"api_documentation/#using-the-generator","title":"Using the Generator","text":""},{"location":"api_documentation/#basic-usage","title":"Basic Usage","text":"<p>To generate API documentation:</p> <pre><code># Using the helper script (recommended)\n./api_docs_helper.sh generate\n\n# Or run the generator directly\n./docs_create/generate_api_docs.py\n</code></pre> <p>This will create or update Markdown files in the <code>docs/api/</code> directory.</p>"},{"location":"api_documentation/#complete-api-documentation-workflow","title":"Complete API Documentation Workflow","text":"<p>Our API documentation helper script provides several commands:</p> <pre><code># Generate API documentation\n./api_docs_helper.sh generate\n\n# Generate and preview in browser\n./api_docs_helper.sh preview\n\n# Generate, preview, and deploy to GitHub Pages\n./api_docs_helper.sh deploy \"Your commit message\"\n\n# Validate docstrings in the codebase\n./api_docs_helper.sh validate\n\n# Clean generated documentation files\n./api_docs_helper.sh clean\n\n# Show help\n./api_docs_helper.sh help\n</code></pre>"},{"location":"api_documentation/#writing-docstrings","title":"Writing Docstrings","text":"<p>For best results, use Google-style docstrings in your code:</p>"},{"location":"api_documentation/#functions-and-methods","title":"Functions and Methods","text":"<pre><code>def function_name(param1: str, param2: int = 10) -&gt; bool:\n    \"\"\"Short description of the function.\n\n    Longer description that explains what the function does.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2, with default value\n\n    Returns:\n        Description of the return value\n\n    Raises:\n        ValueError: When something goes wrong\n\n    Examples:\n        &gt;&gt;&gt; function_name(\"test\", 5)\n        True\n\n        You can also include multi-line examples:\n        &gt;&gt;&gt; result = function_name(\"complex\", 42)\n        &gt;&gt;&gt; print(result)\n        True\n    \"\"\"\n</code></pre>"},{"location":"api_documentation/#classes","title":"Classes","text":"<pre><code>class MyClass:\n    \"\"\"Class description.\n\n    More detailed description of the class and its behavior.\n\n    Attributes:\n        attribute1: Description of attribute1\n        attribute2: Description of attribute2\n    \"\"\"\n\n    def __init__(self, param1: str):\n        \"\"\"Initialize the class.\n\n        Args:\n            param1: Description of parameter\n        \"\"\"\n</code></pre>"},{"location":"api_documentation/#type-annotations","title":"Type Annotations","text":"<p>Always include type annotations for parameters and return values:</p> <pre><code>from typing import List, Dict, Optional, Union, Tuple\n\ndef complex_function(\n    items: List[str],\n    options: Dict[str, int],\n    name: Optional[str] = None,\n    mode: Union[str, int] = \"default\"\n) -&gt; Tuple[bool, List[str]]:\n    \"\"\"Function with complex type annotations.\n\n    Args:\n        items: List of items to process\n        options: Dictionary of options\n        name: Optional name parameter\n        mode: Mode of operation, can be string or integer\n\n    Returns:\n        Tuple containing success flag and list of results\n    \"\"\"\n</code></pre>"},{"location":"api_documentation/#integration-with-github-actions","title":"Integration with GitHub Actions","text":"<p>The API documentation generator is integrated with the GitHub Actions workflow. When you push changes to your repository, it will automatically:</p> <ol> <li>Check out your code</li> <li>Generate updated API documentation</li> <li>Build the MkDocs site</li> <li>Deploy to GitHub Pages</li> </ol>"},{"location":"api_documentation/#best-practices","title":"Best Practices","text":"<ol> <li>Write comprehensive docstrings for all public classes, methods, and functions</li> <li>Include type annotations for parameters and return values</li> <li>Use consistent docstring style (Google style recommended)</li> <li>Regenerate documentation when you make significant API changes</li> <li>Preview locally before deploying to ensure everything looks correct</li> <li>Validate docstrings regularly using <code>./api_docs_helper.sh validate</code></li> </ol>"},{"location":"api_documentation/#docstring-style-guide","title":"Docstring Style Guide","text":""},{"location":"api_documentation/#dos","title":"Do's","text":"<p>\u2705 Include a brief one-line summary at the start of each docstring \u2705 Document all parameters with <code>Args:</code> section \u2705 Document return values with <code>Returns:</code> section \u2705 Document exceptions with <code>Raises:</code> section \u2705 Include examples where helpful \u2705 Use type annotations for all parameters and return values  </p>"},{"location":"api_documentation/#donts","title":"Don'ts","text":"<p>\u274c Leave public methods or classes without docstrings \u274c Write overly terse docstrings that don't explain usage \u274c Skip documenting parameters \u274c Mix different docstring styles (stick to Google style)  </p>"},{"location":"api_documentation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api_documentation/#missing-or-incomplete-documentation","title":"Missing or Incomplete Documentation","text":"<p>If some classes or methods are missing documentation:</p> <ol> <li>Check that they have proper docstrings in the source code</li> <li>Run <code>./api_docs_helper.sh validate</code> to find missing docstrings</li> <li>Ensure the class or module is properly imported in the generator script</li> </ol>"},{"location":"api_documentation/#import-errors","title":"Import Errors","text":"<p>If you see import errors when running the generator:</p> <ol> <li>Make sure the package is installed or in your Python path</li> <li>Check that all dependencies are installed</li> <li>If adding new modules, update the imports in the generator script</li> </ol>"},{"location":"api_documentation/#formatting-issues","title":"Formatting Issues","text":"<p>If the documentation doesn't look right:</p> <ol> <li>Check your docstring format and make sure it follows Google style</li> <li>Ensure proper indentation in your docstrings</li> <li>Verify that type annotations are formatted correctly</li> </ol>"},{"location":"architecture/","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                \u2502         \u2502               \u2502\n\u2502  Ax Platform   \u2502         \u2502  AxScheduler  \u2502\n\u2502                \u2502         \u2502               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                          \u2502\n        \u2502 extends                  \u2502 manages\n        \u25bc                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                \u2502         \u2502               \u2502\n\u2502  Ax Trial      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u25ba  \u2502  Trial        \u2502\n\u2502                \u2502         \u2502               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u2502 contains\n                                   \u25bc\n                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502               \u2502\n                           \u2502  Job          \u2502\n                           \u2502               \u2502\n                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u2502 executed by\n                                   \u25bc\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                 \u2502                             \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524        BaseRunner           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502             \u2502                             \u2502            \u2502\n   \u2502             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n   \u2502                                                        \u2502\n   \u25bc                          \u25bc                             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             \u2502      \u2502               \u2502             \u2502               \u2502\n\u2502JobLibRunner \u2502      \u2502  SlurmRunner  \u2502             \u2502 PanDARunner   \u2502\n\u2502             \u2502      \u2502               \u2502             \u2502               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment_options/","title":"Documentation Deployment Options","text":"<p>This guide covers all available options for deploying your MkDocs documentation to GitHub Pages, with pros and cons for each approach.</p>"},{"location":"deployment_options/#interactive-deployment","title":"Interactive Deployment","text":"<p>We've created an interactive script that guides you through the deployment process:</p> <pre><code>./interactive_deploy_docs.sh\n</code></pre> <p>This script will: 1. Check your workspace for uncommitted changes 2. Ask for a commit message 3. Present deployment options with explanations 4. Execute your chosen deployment method</p> <p>This is the recommended approach for most users, especially those new to documentation deployment.</p>"},{"location":"deployment_options/#direct-deployment-no-branch-switching","title":"Direct Deployment (No Branch Switching)","text":"<p>The direct deployment approach avoids switching branches locally, which prevents issues with uncommitted changes.</p>"},{"location":"deployment_options/#option-1-build-and-deploy-in-one-step","title":"Option 1: Build and Deploy in One Step","text":"<pre><code>./docs_create/push_site_to_ghpages.sh \"Your commit message\"\n</code></pre> <p>This script: - Builds the documentation using MkDocs - Creates the gh-pages branch remotely if it doesn't exist - Clones the gh-pages branch to a temporary directory - Copies your built site to this temporary directory - Commits and pushes the changes - Cleans up without touching your working directory</p>"},{"location":"deployment_options/#option-2-deploy-an-existing-site","title":"Option 2: Deploy an Existing Site","text":"<p>If you've already built the site (e.g., with <code>mkdocs build</code>) and want to deploy it without rebuilding:</p> <pre><code>./docs_create/push_existing_site.sh \"Your commit message\"\n</code></pre> <p>This works the same as the previous script but skips the build step.</p>"},{"location":"deployment_options/#pros-and-cons-of-direct-deployment","title":"Pros and Cons of Direct Deployment","text":"<p>Pros: - Works with uncommitted changes in your workspace - No branch switching required - Prevents accidental loss of work - Cleaner approach that isolates deployment from development</p> <p>Cons: - Slightly more complex implementation - Requires creating a temporary directory</p>"},{"location":"deployment_options/#traditional-deployment-branch-switching","title":"Traditional Deployment (Branch Switching)","text":"<p>The traditional approach involves switching to the gh-pages branch locally, updating content, then switching back.</p>"},{"location":"deployment_options/#option-1-with-automatic-stashing","title":"Option 1: With Automatic Stashing","text":"<pre><code>./docs_create/deploy_docs.sh \"Your commit message\"\n</code></pre> <p>This script: - Builds the documentation - Stashes any uncommitted changes - Switches to the gh-pages branch (creates it if needed) - Updates the content - Commits and pushes the changes - Switches back to your original branch - Restores your stashed changes</p>"},{"location":"deployment_options/#option-2-simple-deployment-requires-clean-workspace","title":"Option 2: Simple Deployment (Requires Clean Workspace)","text":"<pre><code>./docs_create/deploy_docs_simple.sh \"Your commit message\"\n</code></pre> <p>This script requires a clean workspace with all changes committed.</p>"},{"location":"deployment_options/#pros-and-cons-of-traditional-deployment","title":"Pros and Cons of Traditional Deployment","text":"<p>Pros: - Well-documented, standard approach - More straightforward implementation - Widely used in the community</p> <p>Cons: - Requires branch switching - Can cause issues with uncommitted changes - Requires stashing/unstashing for safety</p>"},{"location":"deployment_options/#automated-deployment-with-github-actions","title":"Automated Deployment with GitHub Actions","text":"<p>For teams or larger projects, automated deployment via GitHub Actions is recommended.</p> <pre><code>name: Deploy Documentation\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n      - run: pip install -r docs/requirements-docs.txt\n      - run: mkdocs gh-deploy --force\n</code></pre>"},{"location":"deployment_options/#pros-and-cons-of-github-actions","title":"Pros and Cons of GitHub Actions","text":"<p>Pros: - Fully automated on push - No manual deployment required - Consistent build environment - No local setup needed for deployment</p> <p>Cons: - Requires initial GitHub Actions setup - Less control over exact deployment process - May have a delay between push and deployment</p>"},{"location":"deployment_options/#best-practices-for-any-deployment-method","title":"Best Practices for Any Deployment Method","text":"<ol> <li> <p>Preview locally before deploying:    <pre><code>mkdocs serve\n</code></pre></p> </li> <li> <p>Use the requirements file for consistent dependencies:    <pre><code>pip install -r docs/requirements-docs.txt\n</code></pre></p> </li> <li> <p>Include meaningful commit messages describing documentation changes</p> </li> <li> <p>Monitor your deployment by checking your GitHub Pages URL after deployment</p> </li> <li> <p>Add a .nojekyll file to prevent GitHub Pages from processing your site with Jekyll</p> </li> </ol>"},{"location":"deployment_options/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment_options/#documentation-not-updating","title":"Documentation Not Updating","text":"<p>If your GitHub Pages site is not updating after deployment: 1. Check that the gh-pages branch has the latest content 2. Ensure GitHub Pages is configured to use the gh-pages branch 3. Look for any error messages in GitHub Actions (if using) 4. Try forcing a rebuild by making a small change and redeploying</p>"},{"location":"deployment_options/#broken-links-or-missing-content","title":"Broken Links or Missing Content","text":"<p>If your deployed site has broken links or missing content: 1. Check that all relative links are correct 2. Ensure all referenced files are committed to the repository 3. Verify that the site structure matches your navigation configuration in <code>mkdocs.yml</code></p>"},{"location":"deployment_options/#styling-issues","title":"Styling Issues","text":"<p>If your site is missing styles or appears broken: 1. Check that your theme configuration is correct 2. Ensure all CSS files are properly referenced 3. Add <code>use_directory_urls: false</code> to your <code>mkdocs.yml</code> if needed</p>"},{"location":"github_pages_setup/","title":"Setting Up Documentation Hosting on GitHub","text":"<p>This guide explains how to set up hosting for your project documentation on GitHub Pages using MkDocs with the Material theme and a GitBook-like appearance.</p>"},{"location":"github_pages_setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>GitHub repository for your project</li> <li>Python 3.7+ installed</li> <li>Access to GitHub Actions</li> </ul>"},{"location":"github_pages_setup/#step-1-install-mkdocs-and-required-extensions","title":"Step 1: Install MkDocs and Required Extensions","text":"<p>First, create a requirements file for documentation dependencies (requirements-docs.txt):</p> <pre><code># Create a requirements file for documentation\ncat &gt; docs/requirements-docs.txt &lt;&lt; EOF\nmkdocs&gt;=1.4.0\nmkdocs-material&gt;=8.5.0\npymdown-extensions&gt;=9.0\ncairosvg&gt;=2.5.0\npillow&gt;=9.0.0\npygments&gt;=2.14.0\nEOF\n\n# Install the requirements\npip install -r docs/requirements-docs.txt\n</code></pre>"},{"location":"github_pages_setup/#step-2-create-documentation-directory-structure","title":"Step 2: Create Documentation Directory Structure","text":"<p>Create a <code>docs</code> directory in the root of your repository with the following structure:</p> <pre><code>docs/\n\u251c\u2500\u2500 index.md                  # Home page\n\u251c\u2500\u2500 installation.md           # Installation guide\n\u251c\u2500\u2500 quickstart.md             # Quick start guide\n\u251c\u2500\u2500 architecture.md           # Architecture overview\n\u251c\u2500\u2500 requirements-docs.txt     # Documentation dependencies\n\u251c\u2500\u2500 assets/                   # Images and other assets\n\u2502   \u251c\u2500\u2500 logo.png\n\u2502   \u2514\u2500\u2500 favicon.png\n\u251c\u2500\u2500 stylesheets/              # Custom CSS\n\u2502   \u2514\u2500\u2500 gitbook.css\n\u251c\u2500\u2500 api/                      # API documentation\n\u2502   \u2514\u2500\u2500 index.md\n\u2514\u2500\u2500 tutorials/                # Tutorials\n    \u2514\u2500\u2500 index.md\n</code></pre> <p>Create the base structure with:</p> <pre><code># Create the directory structure\nmkdir -p docs/assets docs/stylesheets docs/api docs/tutorials\n</code></pre>"},{"location":"github_pages_setup/#step-3-create-mkdocs-configuration","title":"Step 3: Create MkDocs Configuration","text":"<p>Create a file named <code>mkdocs.yml</code> in the root of your repository:</p> <pre><code>site_name: Scheduler for AID2E\nsite_description: A Python library for scheduling and managing optimization trials for the ePIC EIC detector design using Ax\nsite_author: AID2E Team\nsite_url: https://aid2e.github.io/scheduler_epic/\nrepo_url: https://github.com/aid2e/scheduler_epic\nrepo_name: aid2e/scheduler_epic\n\n# Theme configuration\ntheme:\n  name: material\n  logo: assets/logo.png\n  favicon: assets/favicon.png\n  palette:\n    # Light mode\n    - media: \"(prefers-color-scheme: light)\"\n      scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/weather-night\n        name: Switch to dark mode\n    # Dark mode\n    - media: \"(prefers-color-scheme: dark)\"\n      scheme: slate\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/weather-sunny\n        name: Switch to light mode\n  features:\n    - navigation.tabs\n    - navigation.sections\n    - navigation.expand\n    - navigation.indexes\n    - navigation.top\n    - toc.follow\n    - content.code.copy\n    - content.code.annotate\n    - content.action.edit\n    - search.highlight\n    - search.suggest\n  icon:\n    repo: fontawesome/brands/github\n\n# Extra configuration\nextra:\n  social:\n    - icon: fontawesome/brands/github\n      link: https://github.com/aid2e/scheduler_epic\n      name: AID2E Scheduler on GitHub\n\n  # Add GitHub edit capabilities\n  repo_icon: github\n  edit_uri: edit/main/docs/\n\n# Add custom CSS to make it more GitBook-like\nextra_css:\n  - stylesheets/gitbook.css\n\n# Markdown extensions for richer content\nmarkdown_extensions:\n  - admonition\n  - attr_list\n  - def_list\n  - footnotes\n  - md_in_html\n  - toc:\n      permalink: true\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.superfences\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.tabbed:\n      alternate_style: true \n  - pymdownx.tasklist:\n      custom_checkbox: true\n</code></pre> <p>This configuration includes: - Material theme with light/dark mode toggle - Custom GitBook-like styling - GitHub repository integration - Enhanced navigation features - Syntax highlighting and other Markdown extensions</p> <p>nav:   - Home: index.md   - Installation: installation.md   - Quick Start: quickstart.md   - Tutorials:     - tutorials/index.md     - Basic Detector Optimization: tutorials/detector_optimization.md     - Slurm Execution: tutorials/slurm_execution.md     - Container-Based Optimization: tutorials/container_based_optimization.md     - Batch Trial Submission: tutorials/batch_trial_submission.md   - API Reference:     - api/index.md     - AxScheduler: api/ax_scheduler.md     - Trial: api/trial.md     - Job: api/job.md     - Runners: api/runners.md   - Architecture: architecture.md <pre><code>## Step 4: Preview Documentation Locally\n\nYou can preview your documentation locally by running:\n\n```bash\nmkdocs serve\n</code></pre></p> <p>This will start a local server at http://127.0.0.1:8000/ where you can preview your documentation.</p>"},{"location":"github_pages_setup/#step-5-configure-github-actions-for-deployment","title":"Step 5: Configure GitHub Actions for Deployment","text":"<p>Create a GitHub Actions workflow file at <code>.github/workflows/deploy-docs.yml</code>:</p> <pre><code>name: Deploy Documentation\n\non:\n  push:\n    branches:\n      - main\n\n# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages\npermissions:\n  contents: write\n  pages: write\n  id-token: write\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          # Install Cairo for SVG support\n          sudo apt-get update\n          sudo apt-get install -y libcairo2-dev pkg-config python3-dev\n          # Install documentation requirements\n          pip install -r docs/requirements-docs.txt\n\n      - name: Configure Git\n        run: |\n          git config --global user.name \"github-actions[bot]\"\n          git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n\n      - name: Deploy documentation\n        run: |\n          # Try up to 3 times to deploy, handling potential race conditions\n          max_attempts=3\n          attempt=1\n\n          while [ $attempt -le $max_attempts ]; do\n            echo \"Deployment attempt $attempt of $max_attempts\"\n\n            if mkdocs gh-deploy --force --clean; then\n              echo \"Deployment successful!\"\n              break\n            else\n              if [ $attempt -eq $max_attempts ]; then\n                echo \"Failed all $max_attempts deployment attempts\"\n                exit 1\n              fi\n\n              echo \"Deployment attempt failed. Fetching latest changes and retrying...\"\n              git fetch origin gh-pages || true\n              sleep 5\n            fi\n\n            attempt=$((attempt+1))\n          done\n</code></pre> <p>This workflow includes retry logic to handle potential race conditions during deployment, which can sometimes occur with GitHub Pages.</p>"},{"location":"github_pages_setup/#step-6-deploy-to-github-pages","title":"Step 6: Deploy to GitHub Pages","text":"<p>There are three ways to deploy your documentation to GitHub Pages:</p>"},{"location":"github_pages_setup/#option-1-automated-deployment-with-github-actions","title":"Option 1: Automated Deployment with GitHub Actions","text":"<ol> <li>Go to your repository on GitHub</li> <li>Navigate to Settings &gt; Pages</li> <li>Under \"Source\", select \"GitHub Actions\"</li> <li>Commit and push your changes to the main branch</li> </ol> <p>After pushing your changes, GitHub Actions will automatically build and deploy your documentation to GitHub Pages.</p>"},{"location":"github_pages_setup/#option-2-direct-deployment-without-branch-switching","title":"Option 2: Direct Deployment without Branch Switching","text":"<p>This approach avoids switching branches locally, which prevents issues with uncommitted changes:</p> <pre><code># Build and deploy in one step\n./docs_create/push_site_to_ghpages.sh \"Update documentation\"\n\n# Or if you've already built the site\n./docs_create/push_existing_site.sh \"Deploy existing site\"\n</code></pre> <p>This approach: 1. Builds the documentation (for push_site_to_ghpages.sh) 2. Creates the gh-pages branch remotely if it doesn't exist 3. Clones the gh-pages branch to a temporary directory 4. Copies your built site to this temporary directory 5. Commits and pushes the changes 6. Cleans up without touching your working directory</p>"},{"location":"github_pages_setup/#option-3-manual-deployment-with-branch-switching","title":"Option 3: Manual Deployment with Branch Switching","text":"<p>If you prefer to manually deploy your documentation:</p> <ol> <li> <p>Build the documentation:    <pre><code>mkdocs build\n</code></pre></p> </li> <li> <p>Deploy the static site to the gh-pages branch:    <pre><code># Make sure to commit your changes first\ngit add .\ngit commit -m \"Update documentation content\"\n\n# Switch to gh-pages branch (create if it doesn't exist)\ngit checkout gh-pages || git checkout --orphan gh-pages\n\n# Remove existing content\nfind . -maxdepth 1 -not -path \"./.git\" -not -path \".\" -exec rm -rf {} \\;\n\n# Copy the built site\ncp -r site/* .\n\n# Add a .nojekyll file to disable Jekyll processing\ntouch .nojekyll\n\n# Commit and push\ngit add .\ngit commit -m \"Update documentation\"\ngit push origin gh-pages\n\n# Switch back to original branch\ngit checkout -\n</code></pre></p> </li> </ol> <p>Note: If you have uncommitted changes, you'll need to commit or stash them before switching branches. You can use <code>git stash</code> before switching branches and <code>git stash pop</code> after returning to your original branch.</p> <p>Your documentation will be available at:</p> <pre><code>https://&lt;username&gt;.github.io/&lt;repository&gt;/\n</code></pre>"},{"location":"github_pages_setup/#step-7-add-a-link-to-your-documentation-in-readme","title":"Step 7: Add a Link to Your Documentation in README","text":"<p>Update your README.md to include a link to your documentation:</p> <pre><code>## Documentation\n\nComprehensive documentation is available at: https://aid2e.github.io/scheduler_epic/\n</code></pre>"},{"location":"github_pages_setup/#updating-documentation","title":"Updating Documentation","text":"<p>To update your documentation:</p> <ol> <li>Make changes to your Markdown files in the <code>docs/</code> directory</li> <li>Commit and push your changes to the main branch</li> <li>GitHub Actions will automatically rebuild and deploy your documentation</li> </ol>"},{"location":"github_pages_setup/#additional-configuration-options","title":"Additional Configuration Options","text":""},{"location":"github_pages_setup/#removing-the-left-navigation-sidebar","title":"Removing the Left Navigation Sidebar","text":"<p>If you want to remove the left navigation sidebar to reduce redundancy, modify the <code>features</code> section in your <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  features:\n    # Remove these lines to disable the left sidebar\n    # - navigation.sections\n    # - navigation.expand\n    # - navigation.indexes\n\n    # Keep these features\n    - navigation.top\n    - toc.follow\n    - content.code.copy\n    - content.code.annotate\n    - content.action.edit\n    - search.highlight\n    - search.suggest\n    - navigation.tabs\n</code></pre> <p>You can also add custom CSS to hide the sidebar completely by adding this to <code>docs/stylesheets/gitbook.css</code>:</p> <pre><code>/* Hide the left sidebar completely */\n.md-sidebar--primary {\n  display: none !important;\n}\n\n/* Adjust the main content width when sidebar is hidden */\n.md-content {\n  max-width: 1000px;\n  margin: 0 auto;\n}\n</code></pre>"},{"location":"github_pages_setup/#adding-a-custom-domain","title":"Adding a Custom Domain","text":"<p>If you want to use a custom domain for your documentation:</p> <ol> <li>Go to your repository's Settings &gt; Pages</li> <li>Under \"Custom domain\", enter your domain name and save</li> <li>Create a CNAME file in the <code>docs/</code> directory with your domain name</li> </ol>"},{"location":"github_pages_setup/#adding-search-functionality","title":"Adding Search Functionality","text":"<p>MkDocs Material theme includes search functionality by default. You can customize it in the <code>mkdocs.yml</code> file:</p> <pre><code>plugins:\n  - search:\n      lang: en\n</code></pre>"},{"location":"github_pages_setup/#adding-analytics","title":"Adding Analytics","text":"<p>You can add Google Analytics to track documentation usage:</p> <pre><code>extra:\n  analytics:\n    provider: google\n    property: G-XXXXXXXXXX\n</code></pre> <p>Replace <code>G-XXXXXXXXXX</code> with your Google Analytics property ID.</p>"},{"location":"github_pages_setup/#gitbook-like-styling","title":"GitBook-Like Styling","text":"<p>To achieve a GitBook-like appearance for our documentation, we've added custom CSS. Create a file at <code>docs/stylesheets/gitbook.css</code>:</p> <pre><code>/* GitBook-like styles */\n:root {\n  --md-primary-fg-color: #4051b5;\n  --md-primary-fg-color--light: #7880c3;\n  --md-primary-fg-color--dark: #303fa1;\n}\n\n/* Make navigation sidebar more like GitBook */\n.md-sidebar--primary {\n  background-color: #fafafa;\n}\n\n[data-md-color-scheme=\"slate\"] .md-sidebar--primary {\n  background-color: #1e1e1e;\n}\n\n/* Improve readability of main content */\n.md-content {\n  max-width: 800px;\n  margin: 0 auto;\n  padding: 1rem 2rem;\n}\n\n/* Enhance code blocks */\n.highlight pre {\n  border-radius: 4px;\n}\n\n/* Make headings more prominent */\n.md-content h1 {\n  font-weight: 600;\n  margin-bottom: 2rem;\n  padding-bottom: 0.5rem;\n  border-bottom: 1px solid rgba(0, 0, 0, 0.1);\n}\n\n[data-md-color-scheme=\"slate\"] .md-content h1 {\n  border-bottom: 1px solid rgba(255, 255, 255, 0.1);\n}\n\n/* Nice link styling */\n.md-content a:not(.md-button) {\n  color: var(--md-primary-fg-color);\n  text-decoration: none;\n  border-bottom: 1px solid transparent;\n  transition: border-color 0.2s ease;\n}\n\n.md-content a:not(.md-button):hover {\n  border-bottom-color: var(--md-primary-fg-color);\n}\n</code></pre> <p>Then reference this CSS file in your <code>mkdocs.yml</code>:</p> <pre><code>extra_css:\n  - stylesheets/gitbook.css\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>This guide covers how to install the Scheduler library for AID2E.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7 or later</li> <li>pip package manager</li> </ul>"},{"location":"installation/#basic-installation","title":"Basic Installation","text":"<p>You can install the Scheduler library from source:</p> <pre><code># Clone the repository\ngit clone https://github.com/aid2e/scheduler_epic\ncd scheduler\n\n# Install the basic package\npip install -e .\n</code></pre>"},{"location":"installation/#installation-with-specific-backends","title":"Installation with Specific Backends","text":"<p>The Scheduler supports different execution backends. You can install the dependencies for the ones you need:</p>"},{"location":"installation/#with-slurm-support","title":"With Slurm Support","text":"<pre><code>pip install -e .[slurm]\n</code></pre>"},{"location":"installation/#with-panda-support","title":"With PanDA Support","text":"<pre><code>pip install -e .[panda]\n</code></pre>"},{"location":"installation/#with-all-features","title":"With All Features","text":"<pre><code>pip install -e .[slurm,panda]\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For development purposes, you may want to install the development dependencies:</p> <pre><code>pip install -e .[dev]\n</code></pre> <p>or</p> <pre><code>pip install -r requirements-dev.txt\n</code></pre>"},{"location":"installation/#verifying-installation","title":"Verifying Installation","text":"<p>You can verify that the installation was successful by running:</p> <pre><code>import scheduler\nprint(scheduler.__version__)\n</code></pre> <p>This should print the version number of the installed package.</p>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Local Execution (JobLibRunner): Any system with Python and sufficient RAM/CPU for your tasks</li> <li>Slurm Execution (SlurmRunner): Access to a Slurm cluster</li> <li>PanDA Execution (PanDARunner): Access to the PanDA distributed computing system</li> </ul>"},{"location":"overview/","title":"Project Overview","text":"<p>A Python library for scheduling and managing optimization trials for the ePIC EIC detector design using Ax, with support for multiple execution backends and job types.</p>"},{"location":"overview/#features","title":"Features","text":"<ul> <li>Direct integration with Ax for optimization</li> <li>Support for multiple job types:</li> <li>Python functions</li> <li>Shell/Python scripts</li> <li>Containers (Docker/Singularity)</li> <li>Multiple execution backends:</li> <li>JobLib (local parallel execution)</li> <li>Slurm (cluster computing)</li> <li>PanDA (distributed computing)</li> <li>Trial and job state monitoring</li> <li>Synchronous or asynchronous execution</li> <li>Batch trial submission for parallel exploration</li> <li>Experiment saving and loading</li> </ul>"},{"location":"overview/#installation","title":"Installation","text":"<pre><code># Basic installation\npip install -e .\n\n# With Slurm support\npip install -e .[slurm]\n\n# With PanDA support\npip install -e .[panda]\n</code></pre>"},{"location":"overview/#usage-examples","title":"Usage Examples","text":""},{"location":"overview/#basic-function-based-optimization","title":"Basic Function-based Optimization","text":"<pre><code>from ax.service.ax_client import AxClient\nfrom scheduler import AxScheduler, JobLibRunner\n\n# Initialize Ax client\nax_client = AxClient()\n\n# Define your parameter space\nax_client.create_experiment(\n    name=\"my_experiment\",\n    parameters=[\n        {\n            \"name\": \"x\",\n            \"type\": \"range\",\n            \"bounds\": [0.0, 1.0],\n            \"value_type\": \"float\",\n        },\n        {\n            \"name\": \"y\",\n            \"type\": \"range\",\n            \"bounds\": [0.0, 1.0],\n            \"value_type\": \"float\",\n        },\n    ],\n    objectives={\"objective\": \"minimize\"},\n)\n\n# Define your objective function\ndef objective_function(parameterization):\n    x = parameterization[\"x\"]\n    y = parameterization[\"y\"]\n    return {\"objective\": (x - 0.5)**2 + (y - 0.5)**2}\n\n# Create a runner\nrunner = JobLibRunner(n_jobs=-1)  # Use all available cores\n\n# Create the scheduler\nscheduler = AxScheduler(ax_client, runner)\n\n# Set the objective function\nscheduler.set_objective_function(objective_function)\n\n# Run the optimization\nbest_params = scheduler.run_optimization(max_trials=10)\nprint(\"Best parameters:\", best_params)\n</code></pre>"},{"location":"overview/#script-based-optimization","title":"Script-based Optimization","text":"<pre><code>from scheduler import AxScheduler, JobLibRunner\n\n# Create a scheduler\nscheduler = AxScheduler(ax_client, runner)\n\n# Use a script as the objective function\nscheduler.set_script_objective(\"./my_simulation_script.py\")\n\n# Run the optimization\nbest_params = scheduler.run_optimization(max_trials=10)\n</code></pre>"},{"location":"overview/#container-based-optimization","title":"Container-based Optimization","text":"<pre><code>from scheduler import AxScheduler, SlurmRunner\n\n# Create a Slurm runner for container execution\nrunner = SlurmRunner(\n    partition=\"compute\",\n    time_limit=\"01:00:00\",\n    memory=\"4G\",\n    cpus_per_task=4,\n    config={\n        'modules': ['singularity']\n    }\n)\n\n# Create the scheduler\nscheduler = AxScheduler(ax_client, runner)\n\n# Use a container as the objective function\nscheduler.set_container_objective(\n    container_image=\"my/simulation:latest\",\n    container_command=\"python /app/simulate.py\"\n)\n\n# Run the optimization\nbest_params = scheduler.run_optimization(max_trials=10)\n</code></pre>"},{"location":"overview/#batch-trial-submission","title":"Batch Trial Submission","text":"<pre><code># Create a batch of trials to run in parallel\nwith scheduler.batch_trial_context() as batch:\n    # Add trials with specific parameter values\n    batch.add_trial({\"x\": 0.1, \"y\": 0.2})\n    batch.add_trial({\"x\": 0.3, \"y\": 0.4})\n    batch.add_trial({\"x\": 0.5, \"y\": 0.6})\n\n    # The trials will be run when exiting the context\n</code></pre>"},{"location":"overview/#saving-and-loading-experiments","title":"Saving and Loading Experiments","text":"<pre><code># Save the experiment to a file\nscheduler.save_experiment(\"my_experiment.json\")\n\n# Load the experiment from a file\nnew_scheduler = AxScheduler(None, runner)\nnew_scheduler.load_experiment(\"my_experiment.json\")\n</code></pre>"},{"location":"overview/#advanced-configuration","title":"Advanced Configuration","text":"<p>The scheduler and runners support various configuration options:</p> <pre><code># Configure the scheduler\nscheduler = AxScheduler(\n    ax_client, \n    runner,\n    config={\n        'monitoring_interval': 10,  # Seconds between status checks\n        'job_output_dir': './outputs',  # Directory for job outputs\n        'synchronous': True,  # Run trials synchronously\n        'cleanup_after_completion': True  # Clean up files after completion\n    }\n)\n\n# Configure the JobLib runner\njoblib_runner = JobLibRunner(\n    n_jobs=-1,\n    backend='loky',\n    config={\n        'container_engine': 'docker',  # or 'singularity'\n        'tmp_dir': './tmp'  # Directory for temporary files\n    }\n)\n\n# Configure the Slurm runner\nslurm_runner = SlurmRunner(\n    partition=\"compute\",\n    time_limit=\"01:00:00\",\n    memory=\"4G\",\n    cpus_per_task=4,\n    config={\n        'modules': ['python', 'singularity'],  # Modules to load\n        'sbatch_options': {\n            'account': 'my-project',\n            'mail-user': 'user@example.com',\n            'mail-type': 'END,FAIL'\n        }\n    }\n)\n</code></pre>"},{"location":"overview/#examples","title":"Examples","text":"<p>See the <code>examples</code> directory for more advanced usage examples:</p> <ul> <li><code>detector_optimization.py</code> - Basic function-based optimization</li> <li><code>enhanced_detector_optimization.py</code> - Advanced function-based optimization</li> <li><code>slurm_optimization.py</code> - Optimization using Slurm for compute</li> <li><code>container_detector_optimization.py</code> - Container-based optimization</li> </ul>"},{"location":"overview/#further-resources","title":"Further Resources","text":"<ul> <li>Installation Guide - Detailed installation instructions</li> <li>Quick Start Guide - Get up and running quickly</li> <li>Tutorials - Step-by-step tutorials for common tasks</li> <li>API Reference - Detailed API documentation</li> <li>Architecture Overview - Understanding the system design</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":"<p>This guide will help you get started with the Scheduler library quickly. We'll cover the basic workflow for setting up and running an optimization experiment.</p>"},{"location":"quickstart/#basic-workflow","title":"Basic Workflow","text":"<p>The typical workflow for using the Scheduler involves:</p> <ol> <li>Defining your parameter space with Ax</li> <li>Creating a runner for job execution</li> <li>Setting up the scheduler with your objective function</li> <li>Running the optimization</li> </ol>"},{"location":"quickstart/#simple-example","title":"Simple Example","text":"<p>Here's a minimal example that optimizes a simple function:</p> <pre><code>from ax.service.ax_client import AxClient\nfrom scheduler import AxScheduler, JobLibRunner\n\n# 1. Initialize Ax client and define parameter space\nax_client = AxClient()\nax_client.create_experiment(\n    name=\"my_experiment\",\n    parameters=[\n        {\n            \"name\": \"x\",\n            \"type\": \"range\",\n            \"bounds\": [0.0, 1.0],\n            \"value_type\": \"float\",\n        },\n        {\n            \"name\": \"y\",\n            \"type\": \"range\",\n            \"bounds\": [0.0, 1.0],\n            \"value_type\": \"float\",\n        },\n    ],\n    objectives={\"objective\": \"minimize\"},\n)\n\n# 2. Define your objective function\ndef objective_function(parameterization):\n    x = parameterization[\"x\"]\n    y = parameterization[\"y\"]\n    return {\"objective\": (x - 0.5)**2 + (y - 0.5)**2}\n\n# 3. Create a runner for local execution\nrunner = JobLibRunner(n_jobs=-1)  # Use all available cores\n\n# 4. Create the scheduler\nscheduler = AxScheduler(ax_client, runner)\n\n# 5. Set the objective function\nscheduler.set_objective_function(objective_function)\n\n# 6. Run the optimization\nbest_params = scheduler.run_optimization(max_trials=10)\nprint(\"Best parameters:\", best_params)\n</code></pre> <p>This example: - Creates an experiment with two parameters (x and y) - Defines a simple quadratic objective function - Uses the JobLibRunner for local parallel execution - Runs 10 trials to find the optimal parameter values</p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>For more advanced usage, check out:</p> <ul> <li>Tutorial: Detector Optimization</li> <li>Tutorial: Using Slurm for Execution</li> <li>Tutorial: Container-Based Jobs</li> <li>API Reference: AxScheduler</li> <li>API Reference: Runners</li> </ul>"},{"location":"step_by_step_guide/","title":"Step-by-Step Documentation Guide","text":"<p>This guide provides detailed instructions for creating, building, and deploying documentation for your project using MkDocs and GitHub Pages.</p>"},{"location":"step_by_step_guide/#prerequisites","title":"Prerequisites","text":"<ul> <li>Git installed</li> <li>Python 3.7+ installed</li> <li>A GitHub repository for your project</li> </ul>"},{"location":"step_by_step_guide/#step-1-set-up-documentation-structure","title":"Step 1: Set Up Documentation Structure","text":""},{"location":"step_by_step_guide/#option-a-using-the-automatic-generator","title":"Option A: Using the Automatic Generator","text":"<p>If you're starting from scratch, use the provided documentation generator script:</p> <pre><code>cd /mnt/d/AID2E/scheduler_epic\n# Make the script executable if needed\nchmod +x docs_create/generate_docs.sh\n# Run the generator (replace with your project details)\n./docs_create/generate_docs.sh \"Your Project Name\" \"username/repository\"\n</code></pre>"},{"location":"step_by_step_guide/#option-b-manual-setup","title":"Option B: Manual Setup","text":"<p>If you prefer to set up manually:</p> <ol> <li>Create a docs directory and basic structure:</li> </ol> <pre><code>mkdir -p docs/assets docs/stylesheets docs/api docs/tutorials\n</code></pre> <ol> <li>Create a requirements file for documentation:</li> </ol> <pre><code>cat &gt; docs/requirements-docs.txt &lt;&lt; EOF\nmkdocs&gt;=1.4.0\nmkdocs-material&gt;=8.5.0\npymdown-extensions&gt;=9.0\ncairosvg&gt;=2.5.0\npillow&gt;=9.0.0\npygments&gt;=2.14.0\nEOF\n</code></pre> <ol> <li>Create a basic mkdocs.yml configuration in your project root:</li> </ol> <p>You can also use <code>generate_docs.sh</code> to generate the mkdocs.yml file which is located in <code>docs_create</code> folder. <pre><code>cat &gt; mkdocs.yml &lt;&lt; EOF\nsite_name: Project NAME\nsite_description: Project description\nsite_author: AID2E Team\nsite_url: SITE URL\nrepo_url: https://github.com/aid2e/scheduler_epic\nrepo_name: aid2e/scheduler_epic\n\n# Theme configuration\ntheme:\n  name: material\n  logo: assets/logo.png\n  favicon: assets/favicon.png\n  palette:\n    # Light mode\n    - media: \"(prefers-color-scheme: light)\"\n      scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/weather-night\n        name: Switch to dark mode\n    # Dark mode\n    - media: \"(prefers-color-scheme: dark)\"\n      scheme: slate\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/weather-sunny\n        name: Switch to light mode\n  features:\n    # Your existing features\n    - navigation.top\n    - toc.follow\n    - content.code.copy\n    - content.code.annotate\n    - content.action.edit\n    - search.highlight\n    - search.suggest\n    - navigation.tabs\n    # Add these features to enable the left sidebar\n    - navigation.sections\n    - navigation.indexes\n    - navigation.expand\n    # Keep these existing features\n    - navigation.tabs.sticky\n    - navigation.tabs.sticky_header\n    - navigation.tabs.instant\n    - navigation.tabs.autohide\n    - navigation.instant\n    - header.autohide\n  icon:\n    repo: fontawesome/brands/github\n\n# Extra configuration\nextra:\n  social:\n    - icon: fontawesome/brands/github\n      link: https://github.com/aid2e/scheduler_epic\n      name: AID2E Scheduler on GitHub\n\n  # Add GitHub edit capabilities\n  repo_icon: github\n  edit_uri: edit/main/docs/\n\n# Markdown extensions for richer content\nmarkdown_extensions:\n  - admonition\n  - attr_list\n  - def_list\n  - footnotes\n  - md_in_html\n  - toc:\n      permalink: true\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.superfences\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.tabbed:\n      alternate_style: true \n  - pymdownx.tasklist:\n      custom_checkbox: true\nEOF\n</code></pre></p>"},{"location":"step_by_step_guide/#step-2-install-documentation-requirements","title":"Step 2: Install Documentation Requirements","text":"<p>Install the required packages for building the documentation:</p> <pre><code>pip install -r docs/requirements-docs.txt\n</code></pre>"},{"location":"step_by_step_guide/#step-3-create-basic-documentation-content","title":"Step 3: Create Basic Documentation Content","text":"<p>At minimum, you need an index.md file in your docs directory:</p> <pre><code>cat &gt; docs/index.md &lt;&lt; EOF\n# Project Documentation\n\nWelcome to the project documentation.\n\n## Overview\n\nDescribe your project here.\n\n## Getting Started\n\n- [Installation](installation.md)\n- [Quick Start](quickstart.md)\nEOF\n</code></pre>"},{"location":"step_by_step_guide/#step-4-add-documentation-for-your-project","title":"Step 4: Add Documentation for Your Project","text":"<ol> <li>Create documentation files for your project components</li> <li>Organize them in appropriate directories (tutorials, API reference, etc.)</li> <li>Update the navigation structure in mkdocs.yml</li> </ol> <p>Example for adding a tutorial:</p> <pre><code>cat &gt; docs/tutorials/index.md &lt;&lt; EOF\n# Tutorials\n\nThis section contains tutorials for using the project.\n\n## Available Tutorials\n\n- [Tutorial 1](tutorial1.md)\nEOF\n</code></pre>"},{"location":"step_by_step_guide/#automatically-generate-api-documentation","title":"Automatically Generate API Documentation","text":"<p>To automatically generate API documentation from your source code:</p> <ol> <li>Use the provided API documentation generator script:</li> </ol> <pre><code># Make the script executable\nchmod +x docs_create/generate_api_docs.py\n\n# Generate API documentation\n./docs_create/generate_api_docs.py\n</code></pre> <p>This script: - Parses your Python modules and classes - Extracts docstrings and signatures - Generates formatted Markdown files in the <code>docs/api/</code> directory - Creates a consistent API reference with proper formatting</p> <ol> <li>For convenience, you can use the update script that also previews and deploys:</li> </ol> <pre><code>./update_api_docs.sh \"Update API documentation\"\n</code></pre> <p>This automatically: - Generates the API documentation - Starts a preview server so you can review the changes - Optionally deploys to GitHub Pages when you're satisfied</p>"},{"location":"step_by_step_guide/#step-5-preview-your-documentation-locally","title":"Step 5: Preview Your Documentation Locally","text":"<p>Preview your documentation to check how it looks:</p> <pre><code>cd /mnt/d/AID2E/scheduler_epic\nmkdocs serve\n</code></pre> <p>This will start a local server at http://127.0.0.1:8000/ where you can preview your documentation.</p>"},{"location":"step_by_step_guide/#step-6-build-your-documentation","title":"Step 6: Build Your Documentation","text":"<p>Once you're satisfied with your documentation, build it:</p> <pre><code>cd /mnt/d/AID2E/scheduler_epic\nmkdocs build\n</code></pre> <p>This will create a <code>site</code> directory containing the static HTML files.</p>"},{"location":"step_by_step_guide/#step-7-deploy-to-github-pages","title":"Step 7: Deploy to GitHub Pages","text":"<p>There are multiple options for deploying your documentation to GitHub Pages:</p>"},{"location":"step_by_step_guide/#option-a-using-github-actions-automated-deployment","title":"Option A: Using GitHub Actions (Automated Deployment)","text":"<ol> <li>Create a GitHub Actions workflow file:</li> </ol> <pre><code>mkdir -p .github/workflows\ncat &gt; .github/workflows/deploy-docs.yml &lt;&lt; EOF\nname: Deploy Documentation\n\non:\n  push:\n    branches:\n      - main\n\n# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages\npermissions:\n  contents: write\n  pages: write\n  id-token: write\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          # Install Cairo for SVG support\n          sudo apt-get update\n          sudo apt-get install -y libcairo2-dev pkg-config python3-dev\n          # Install documentation requirements\n          pip install -r docs/requirements-docs.txt\n\n      - name: Configure Git\n        run: |\n          git config --global user.name \"github-actions[bot]\"\n          git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n\n      - name: Generate API Documentation\n        run: |\n          # Generate API documentation from source code\n          python docs_create/generate_api_docs.py\n\n      - name: Deploy documentation\n        run: |\n          # Try up to 3 times to deploy, handling potential race conditions\n          max_attempts=3\n          attempt=1\n\n          while [ \\$attempt -le \\$max_attempts ]; do\n            echo \"Deployment attempt \\$attempt of \\$max_attempts\"\n\n            if mkdocs gh-deploy --force --clean; then\n              echo \"Deployment successful!\"\n              break\n            else\n              if [ \\$attempt -eq \\$max_attempts ]; then\n                echo \"Failed all \\$max_attempts deployment attempts\"\n                exit 1\n              fi\n\n              echo \"Deployment attempt failed. Fetching latest changes and retrying...\"\n              git fetch origin gh-pages || true\n              sleep 5\n            fi\n\n            attempt=\\$((attempt+1))\n          done\nEOF\n</code></pre> <ol> <li>Push your changes to GitHub:</li> </ol> <pre><code>git add .\ngit commit -m \"Add documentation and GitHub Actions workflow\"\ngit push origin main\n</code></pre> <ol> <li>Configure GitHub Pages in your repository settings to use the gh-pages branch.</li> </ol>"},{"location":"step_by_step_guide/#option-b-direct-deployment-without-branch-switching","title":"Option B: Direct Deployment without Branch Switching","text":"<p>This approach avoids switching branches locally, which prevents issues with uncommitted changes. Instead, it clones the gh-pages branch to a temporary directory, updates it with your built site, and pushes it back:</p> <pre><code>cd /mnt/d/AID2E/scheduler_epic\n./docs_create/push_site_to_ghpages.sh \"Update documentation\"\n</code></pre> <p>This script: 1. Builds the documentation 2. Creates the gh-pages branch remotely if it doesn't exist 3. Clones the gh-pages branch to a temporary directory 4. Copies your built site to this temporary directory 5. Commits and pushes the changes 6. Cleans up the temporary directory</p> <p>If you've already built the site and don't want to rebuild:</p> <pre><code>cd /mnt/d/AID2E/scheduler_epic\n./docs_create/push_existing_site.sh \"Deploy existing site\"\n</code></pre>"},{"location":"step_by_step_guide/#option-c-traditional-deployment-with-branch-switching","title":"Option C: Traditional Deployment with Branch Switching","text":""},{"location":"step_by_step_guide/#option-c-traditional-deployment-with-branch-switching_1","title":"Option C: Traditional Deployment with Branch Switching","text":"<p>You can manually deploy your documentation by switching branches:</p>"},{"location":"step_by_step_guide/#using-the-standard-deployment-script","title":"Using the Standard Deployment Script","text":"<p>This script handles uncommitted changes by stashing them:</p> <pre><code>cd /mnt/d/AID2E/scheduler_epic\n./docs_create/deploy_docs.sh \"Update documentation\"\n</code></pre>"},{"location":"step_by_step_guide/#using-the-simple-deployment-script","title":"Using the Simple Deployment Script","text":"<p>This script requires you to commit your changes first:</p> <pre><code>cd /mnt/d/AID2E/scheduler_epic\n# Commit your changes first\ngit add .\ngit commit -m \"Update documentation content\"\n# Then deploy\n./docs_create/deploy_docs_simple.sh \"Deploy documentation\"\n</code></pre>"},{"location":"step_by_step_guide/#emergency-fix-script","title":"Emergency Fix Script","text":"<p>For handling deployment issues:</p> <pre><code>cd /mnt/d/AID2E/scheduler_epic\n./docs_create/fix_deploy.sh \"Fix and deploy documentation\"\n</code></pre>"},{"location":"step_by_step_guide/#step-8-verify-your-documentation","title":"Step 8: Verify Your Documentation","text":"<p>After deployment, your documentation should be available at:</p> <pre><code>https://aid2e.github.io/[repository]/\n</code></pre> <p>Visit this URL to ensure your documentation is correctly deployed.</p>"},{"location":"step_by_step_guide/#step-9-add-a-link-to-your-documentation-in-readme","title":"Step 9: Add a Link to Your Documentation in README","text":"<p>Update your README.md to include a link to your documentation:</p> <pre><code>## Documentation\n\nComprehensive documentation is available at: https://aid2e.github.io/[repository]/\n</code></pre>"},{"location":"step_by_step_guide/#maintenance-and-updates","title":"Maintenance and Updates","text":"<p>To update your documentation:</p> <ol> <li>Make changes to your Markdown files in the <code>docs/</code> directory</li> <li>Preview changes locally with <code>mkdocs serve</code></li> <li>Build with <code>mkdocs build</code></li> <li>Deploy using your preferred method from Step 8</li> </ol>"},{"location":"step_by_step_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"step_by_step_guide/#uncommitted-changes-error","title":"Uncommitted Changes Error","text":"<p>If you see an error about uncommitted changes when deploying:</p> <pre><code>error: Your local changes to the following files would be overwritten by checkout:\n        [list of files]\nPlease commit your changes or stash them before you switch branches.\nAborting\n</code></pre> <p>Use one of these solutions:</p> <ol> <li> <p>Commit your changes before deploying:    <pre><code>git add .\ngit commit -m \"Your message here\"\n</code></pre></p> </li> <li> <p>Use the <code>deploy_docs.sh</code> script which automatically handles stashing and restoring changes</p> </li> <li> <p>Use the emergency fix script:    <pre><code>./docs_create/fix_deploy.sh \"Fix and deploy\"\n</code></pre></p> </li> </ol>"},{"location":"step_by_step_guide/#404-page-not-found","title":"404 Page Not Found","text":"<p>If your site shows a 404 error:</p> <ol> <li>Make sure GitHub Pages is enabled in your repository settings</li> <li>Verify the gh-pages branch exists and contains your documentation</li> <li>Check the source settings in GitHub Pages configuration</li> </ol>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed API documentation for the scheduler package.</p> <p>Tip: Use the search box in the top navigation bar to quickly find specific classes or methods.</p>"},{"location":"api/#core-components","title":"Core Components","text":""},{"location":"api/#axscheduler","title":"AxScheduler","text":"<p>The main entry point for using the Scheduler library. It integrates with Ax for optimization and manages the execution of trials.</p> <pre><code># Example usage\nfrom scheduler import AxScheduler, JobLibRunner\nfrom ax.service.ax_client import AxClient\n\nax_client = AxClient()\n# Set up parameters...\n\nrunner = JobLibRunner()\nscheduler = AxScheduler(ax_client, runner)\nscheduler.set_objective_function(my_objective_function)\nbest_params = scheduler.run_optimization(max_trials=10)\n</code></pre>"},{"location":"api/#trial","title":"Trial","text":"<p>Represents a single optimization trial with parameters and jobs.</p>"},{"location":"api/#job","title":"Job","text":"<p>Represents a single computational job that executes code with specific parameters.</p>"},{"location":"api/#runners","title":"Runners","text":"<p>Runners are responsible for executing jobs on different computing backends.</p>"},{"location":"api/#baserunner","title":"BaseRunner","text":"<p>The abstract base class that defines the interface for all runners.</p>"},{"location":"api/#joblibrunner","title":"JobLibRunner","text":"<p>Runner for local parallel execution using JobLib.</p>"},{"location":"api/#slurmrunner","title":"SlurmRunner","text":"<p>Runner for execution on Slurm clusters.</p>"},{"location":"api/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>BaseRunner\n\u251c\u2500\u2500 JobLibRunner\n\u251c\u2500\u2500 SlurmRunner\n</code></pre>"},{"location":"api/#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<p>Each class documentation page includes:</p> <ol> <li>Class Description - Overview of the class's purpose</li> <li>Class Definition - The constructor signature and parameters</li> <li>Methods Table - Quick reference of all available methods</li> <li>Method Details - Detailed documentation for each method</li> </ol> <p>The documentation is automatically generated from docstrings in the source code.</p>"},{"location":"api/ax_scheduler/","title":"AxScheduler","text":"<p>Defined in <code>scheduler.ax_scheduler</code></p> <p>A scheduler that integrates with Ax for optimization. This scheduler allows running Ax trials using different runners.</p>"},{"location":"api/ax_scheduler/#class-definition","title":"Class Definition","text":"<pre><code>class AxScheduler(self, ax_client_or_experiment: Union[ax.service.ax_client.AxClient, ax.core.experiment.Experiment], runner: &lt;class 'BaseRunner'&gt;, config: Dict[str, Any] = None):\n    \"\"\"\n    Initialize a new AxScheduler.\n    **Args:**\n    * **ax_client_or_experiment**: The Ax client or experiment to use for optimization\n    * **runner**: The runner to use for executing jobs\n    * **config**: Additional configuration options:\n    * **monitoring_interval**: Seconds between monitoring checks (default: 10)\n    * **max_trial_monitoring_time**: Maximum time to monitor a trial in seconds (default: 86400 = 24 hours)\n    * **job_output_dir**: Directory to store job outputs (default: ~/ax_scheduler_output)\n    * **cleanup_after_completion**: Whether to clean up job files after completion (default: False)\n    * **synchronous**: Whether to run trials synchronously (default: False)\n    \"\"\"\n</code></pre>"},{"location":"api/ax_scheduler/#methods","title":"Methods","text":"Method Description <code>batch_trial_context</code> Context manager for creating and running a batch of trials. <code>complete_trial</code> Mark a trial as completed in Ax. <code>get_next_trial</code> Generate a new trial using Ax and return its index. <code>load_experiment</code> Load an experiment from a file. <code>monitor_trials</code> Monitor all running trials. <code>run_optimization</code> Run the optimization process. <code>run_trial</code> Run a specific trial. <code>save_experiment</code> Save the experiment to a file. <code>set_container_objective</code> Set a container to use as the objective function. <code>set_objective_function</code> Set the objective function to optimize. <code>set_script_objective</code> Set a script to use as the objective function."},{"location":"api/ax_scheduler/#method-details","title":"Method Details","text":""},{"location":"api/ax_scheduler/#batch_trial_context","title":"batch_trial_context","text":"<pre><code>def batch_trial_context(self) -&gt; Any\n</code></pre> <p>Context manager for creating and running a batch of trials. This is useful for running multiple trials in parallel. Example:     <pre><code>with scheduler.batch_trial_context() as batch:\n    for i in range(5):\n        params = {'x': i * 0.1, 'y': i * 0.2}\n        batch.add_trial(params)\n    batch.run()\n</code></pre></p>"},{"location":"api/ax_scheduler/#complete_trial","title":"complete_trial","text":"<pre><code>def complete_trial(self, trial_index: &lt;class 'int'&gt;, raw_data: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Mark a trial as completed in Ax. Args: * trial_index: The index of the trial to complete * raw_data: Raw data to attach to the trial</p>"},{"location":"api/ax_scheduler/#get_next_trial","title":"get_next_trial","text":"<pre><code>def get_next_trial(self) -&gt; Optional[int]\n</code></pre> <p>Generate a new trial using Ax and return its index. Returns:   The index of the new trial, or None if no more trials can be generated</p>"},{"location":"api/ax_scheduler/#load_experiment","title":"load_experiment","text":"<pre><code>def load_experiment(self, path: &lt;class 'str'&gt;) -&gt; None\n</code></pre> <p>Load an experiment from a file. Args: * path: Path to load the experiment from</p>"},{"location":"api/ax_scheduler/#monitor_trials","title":"monitor_trials","text":"<pre><code>def monitor_trials(self) -&gt; None\n</code></pre> <p>Monitor all running trials.</p>"},{"location":"api/ax_scheduler/#run_optimization","title":"run_optimization","text":"<pre><code>def run_optimization(self, max_trials: &lt;class 'int'&gt; = 10) -&gt; Dict[str, Any]\n</code></pre> <p>Run the optimization process. Args: * max_trials: Maximum number of trials to run</p> <p>Returns:   The best parameters found</p>"},{"location":"api/ax_scheduler/#run_trial","title":"run_trial","text":"<pre><code>def run_trial(self, trial_index: &lt;class 'int'&gt;) -&gt; &lt;class 'Trial'&gt;\n</code></pre> <p>Run a specific trial. Args: * trial_index: The index of the trial to run</p> <p>Returns:   The Trial object</p>"},{"location":"api/ax_scheduler/#save_experiment","title":"save_experiment","text":"<pre><code>def save_experiment(self, path: &lt;class 'str'&gt;) -&gt; None\n</code></pre> <p>Save the experiment to a file. Args: * path: Path to save the experiment to</p>"},{"location":"api/ax_scheduler/#set_container_objective","title":"set_container_objective","text":"<pre><code>def set_container_objective(self, container_image: &lt;class 'str'&gt;, container_command: Optional[str] = None) -&gt; Any\n</code></pre> <p>Set a container to use as the objective function. Args: * container_image: Container image to run for each trial * container_command: Command to run in the container (optional)</p>"},{"location":"api/ax_scheduler/#set_objective_function","title":"set_objective_function","text":"<pre><code>def set_objective_function(self, objective_fn: Callable[[Dict[str, Any]], Dict[str, Any]]) -&gt; Any\n</code></pre> <p>Set the objective function to optimize. Args: * objective_fn: The objective function to optimize</p>"},{"location":"api/ax_scheduler/#set_script_objective","title":"set_script_objective","text":"<pre><code>def set_script_objective(self, script_path: &lt;class 'str'&gt;) -&gt; Any\n</code></pre> <p>Set a script to use as the objective function. Args: * script_path: Path to the script to run for each trial</p>"},{"location":"api/base_runner/","title":"BaseRunner","text":"<p>Defined in <code>scheduler.runners.base_runner</code></p> <p>Abstract base class for job runners. Runners are responsible for executing jobs on different systems (local, Slurm, PanDA, etc.).</p> <p>Inherits from: ABC</p>"},{"location":"api/base_runner/#class-definition","title":"Class Definition","text":"<pre><code>class BaseRunner(self, config: Dict[str, Any] = None):\n    \"\"\"\n    Initialize a new runner.\n    **Args:**\n    * **config**: Configuration for the runner\n    \"\"\"\n</code></pre>"},{"location":"api/base_runner/#methods","title":"Methods","text":"Method Description <code>cancel_job</code> Cancel a job. <code>check_job_status</code> Check the status of a job and update its state. <code>run_job</code> Run a job."},{"location":"api/base_runner/#method-details","title":"Method Details","text":""},{"location":"api/base_runner/#cancel_job","title":"cancel_job","text":"<pre><code>def cancel_job(self, job: Any) -&gt; None\n</code></pre> <p>Cancel a job. Args: * job: The job to cancel</p>"},{"location":"api/base_runner/#check_job_status","title":"check_job_status","text":"<pre><code>def check_job_status(self, job: Any) -&gt; None\n</code></pre> <p>Check the status of a job and update its state. Args: * job: The job to check</p>"},{"location":"api/base_runner/#run_job","title":"run_job","text":"<pre><code>def run_job(self, job: Any) -&gt; None\n</code></pre> <p>Run a job. Args: * job: The job to run</p>"},{"location":"api/job/","title":"Job","text":"<p>Defined in <code>scheduler.job.job</code></p> <p>A job that can be run by a runner. Each job has a state that is tracked. Jobs can be one of several types: - Function: A Python function to run - Script: A shell or Python script to execute - Container: A container to run</p>"},{"location":"api/job/#class-definition","title":"Class Definition","text":"<pre><code>class Job(self, job_id: &lt;class 'str'&gt;, job_type: &lt;enum 'JobType'&gt; = JobType.FUNCTION, function: Optional[Callable] = None, script_path: Optional[str] = None, container_image: Optional[str] = None, container_command: Optional[str] = None, params: Dict[str, Any] = None, env_vars: Dict[str, str] = None, working_dir: Optional[str] = None, output_files: Optional[List[str]] = None):\n    \"\"\"\n    Initialize a new job.\n    **Args:**\n    * **job_id**: Unique identifier for the job\n    * **job_type**: Type of job (FUNCTION, SCRIPT, or CONTAINER)\n    * **function**: The function to run for this job (if job_type is FUNCTION)\n    * **script_path**: Path to the script to run (if job_type is SCRIPT)\n    * **container_image**: Container image to run (if job_type is CONTAINER)\n    * **container_command**: Command to run in the container (if job_type is CONTAINER)\n    * **params**: Parameters to pass to the function or script\n    * **env_vars**: Environment variables to set for the job\n    * **working_dir**: Working directory for the job\n    * **output_files**: List of output files to collect after job completion\n    \"\"\"\n</code></pre>"},{"location":"api/job/#methods","title":"Methods","text":"Method Description <code>complete</code> Mark the job as completed and store its results. <code>fail</code> Mark the job as failed and store the error. <code>get_results</code> Get the results of this job. <code>has_failed</code> Check if the job has failed. <code>is_completed</code> Check if the job is completed. <code>is_running</code> Check if the job is running. <code>run</code> Run this job using its assigned runner. <code>set_runner</code> Set the runner for this job."},{"location":"api/job/#method-details","title":"Method Details","text":""},{"location":"api/job/#complete","title":"complete","text":"<pre><code>def complete(self, results: Dict[str, Any]) -&gt; None\n</code></pre> <p>Mark the job as completed and store its results. Args: * results: The results of the job</p>"},{"location":"api/job/#fail","title":"fail","text":"<pre><code>def fail(self, error: Optional[str] = None) -&gt; None\n</code></pre> <p>Mark the job as failed and store the error. Args: * error: The error that caused the job to fail</p>"},{"location":"api/job/#get_results","title":"get_results","text":"<pre><code>def get_results(self) -&gt; Dict[str, Any]\n</code></pre> <p>Get the results of this job. Returns:   Dictionary of results</p>"},{"location":"api/job/#has_failed","title":"has_failed","text":"<pre><code>def has_failed(self) -&gt; &lt;class 'bool'&gt;\n</code></pre> <p>Check if the job has failed. Returns:   True if the job has failed, False otherwise</p>"},{"location":"api/job/#is_completed","title":"is_completed","text":"<pre><code>def is_completed(self) -&gt; &lt;class 'bool'&gt;\n</code></pre> <p>Check if the job is completed. Returns:   True if the job is completed, False otherwise</p>"},{"location":"api/job/#is_running","title":"is_running","text":"<pre><code>def is_running(self) -&gt; &lt;class 'bool'&gt;\n</code></pre> <p>Check if the job is running. Returns:   True if the job is running, False otherwise</p>"},{"location":"api/job/#run","title":"run","text":"<pre><code>def run(self) -&gt; None\n</code></pre> <p>Run this job using its assigned runner. Raises: * ValueError: If no runner has been assigned</p>"},{"location":"api/job/#set_runner","title":"set_runner","text":"<pre><code>def set_runner(self, runner: Any) -&gt; None\n</code></pre> <p>Set the runner for this job. Args: * runner: The runner to use for this job</p>"},{"location":"api/joblib_runner/","title":"JobLibRunner","text":"<p>Defined in <code>scheduler.runners.joblib_runner</code></p> <p>A runner that uses joblib for parallel execution. This runner is suitable for local execution with multiple cores. It can handle different job types: - Function: Uses joblib to run Python functions - Script: Executes scripts in separate processes - Container: Runs containers using Docker or Singularity</p> <p>Inherits from: BaseRunner</p>"},{"location":"api/joblib_runner/#class-definition","title":"Class Definition","text":"<pre><code>class JobLibRunner(self, n_jobs: &lt;class 'int'&gt; = -1, backend: &lt;class 'str'&gt; = loky, config: Dict[str, Any] = None):\n    \"\"\"\n    Initialize a new JobLibRunner.\n    **Args:**\n    * **n_jobs**: Number of jobs to run in parallel (-1 for all cores)\n    * **backend**: Backend to use for joblib (loky, threading, multiprocessing)\n    * **config**: Additional configuration options:\n    * **container_engine**: 'docker' or 'singularity' (default: 'docker')\n    * **tmp_dir**: Directory for temporary files (default: system temp dir)\n    \"\"\"\n</code></pre>"},{"location":"api/joblib_runner/#methods","title":"Methods","text":"Method Description <code>cancel_job</code> Cancel a job. <code>check_job_status</code> Check the status of a job and update its state. <code>run_job</code> Run a job using the appropriate execution method. <code>shutdown</code> Shutdown the executor."},{"location":"api/joblib_runner/#method-details","title":"Method Details","text":""},{"location":"api/joblib_runner/#cancel_job","title":"cancel_job","text":"<pre><code>def cancel_job(self, job: Any) -&gt; None\n</code></pre> <p>Cancel a job. Args: * job: The job to cancel</p>"},{"location":"api/joblib_runner/#check_job_status","title":"check_job_status","text":"<pre><code>def check_job_status(self, job: Any) -&gt; None\n</code></pre> <p>Check the status of a job and update its state. Args: * job: The job to check</p>"},{"location":"api/joblib_runner/#run_job","title":"run_job","text":"<pre><code>def run_job(self, job: Any) -&gt; None\n</code></pre> <p>Run a job using the appropriate execution method. Args: * job: The job to run</p>"},{"location":"api/joblib_runner/#shutdown","title":"shutdown","text":"<pre><code>def shutdown(self) -&gt; Any\n</code></pre> <p>Shutdown the executor.</p>"},{"location":"api/runners/","title":"Runners","text":"<p>Runners are responsible for executing jobs on different computing backends.</p>"},{"location":"api/runners/#classes","title":"Classes","text":""},{"location":"api/runners/#baserunner","title":"BaseRunner","text":"<p>Abstract base class for job runners.</p>"},{"location":"api/runners/#joblibrunner","title":"JobLibRunner","text":"<p>A runner that uses joblib for parallel execution.</p>"},{"location":"api/runners/#slurmrunner","title":"SlurmRunner","text":"<p>A runner that submits jobs to a Slurm cluster.</p>"},{"location":"api/runners/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>BaseRunner\n\u251c\u2500\u2500 JobLibRunner\n\u2514\u2500\u2500 SlurmRunner\n</code></pre>"},{"location":"api/runners/#usage-examples","title":"Usage Examples","text":"<pre><code># Using JobLibRunner for local parallel execution\nfrom scheduler import AxScheduler, JobLibRunner\nfrom ax.service.ax_client import AxClient\n\nrunner = JobLibRunner(n_jobs=4)  # Use 4 parallel processes\nscheduler = AxScheduler(ax_client, runner)\n\n# Using SlurmRunner for cluster execution\nfrom scheduler import SlurmRunner\n\nrunner = SlurmRunner(\n    partition='compute',\n    time='1:00:00',\n    memory='4G'\n)\nscheduler = AxScheduler(ax_client, runner)\n</code></pre>"},{"location":"api/slurm_runner/","title":"SlurmRunner","text":"<p>Defined in <code>scheduler.runners.slurm_runner</code></p> <p>A runner that submits jobs to a Slurm cluster. This runner creates temporary job scripts and submits them to Slurm. It can handle different job types: - Function: Serializes and runs Python functions - Script: Executes scripts directly - Container: Runs containers using Singularity</p> <p>Inherits from: BaseRunner</p>"},{"location":"api/slurm_runner/#class-definition","title":"Class Definition","text":"<pre><code>class SlurmRunner(self, partition: &lt;class 'str'&gt; = batch, time_limit: &lt;class 'str'&gt; = 01:00:00, memory: &lt;class 'str'&gt; = 4G, cpus_per_task: &lt;class 'int'&gt; = 1, config: Dict[str, Any] = None):\n    \"\"\"\n    Initialize a new SlurmRunner.\n    **Args:**\n    * **partition**: Slurm partition to submit jobs to\n    * **time_limit**: Time limit for jobs (HH:MM:SS)\n    * **memory**: Memory to allocate per job\n    * **cpus_per_task**: Number of CPUs to allocate per job\n    * **config**: Additional configuration options:\n    * **modules**: List of modules to load (default: ['python'])\n    * **singularity_path**: Path to singularity executable (default: 'singularity')\n    * **job_dir**: Directory to store job files (default: ~/slurm_jobs)\n    \"\"\"\n</code></pre>"},{"location":"api/slurm_runner/#methods","title":"Methods","text":"Method Description <code>cancel_job</code> Cancel a job. <code>check_job_status</code> Check the status of a job and update its state. <code>run_job</code> Submit a job to Slurm."},{"location":"api/slurm_runner/#method-details","title":"Method Details","text":""},{"location":"api/slurm_runner/#cancel_job","title":"cancel_job","text":"<pre><code>def cancel_job(self, job: Any) -&gt; None\n</code></pre> <p>Cancel a job. Args: * job: The job to cancel</p>"},{"location":"api/slurm_runner/#check_job_status","title":"check_job_status","text":"<pre><code>def check_job_status(self, job: Any) -&gt; None\n</code></pre> <p>Check the status of a job and update its state. Args: * job: The job to check</p>"},{"location":"api/slurm_runner/#run_job","title":"run_job","text":"<pre><code>def run_job(self, job: Any) -&gt; None\n</code></pre> <p>Submit a job to Slurm. Args: * job: The job to run</p>"},{"location":"api/trial/","title":"Trial","text":"<p>Defined in <code>scheduler.trial.trial</code></p> <p>A trial class that extends Ax trial functionality. A trial can contain multiple jobs and has a state that is tracked.</p>"},{"location":"api/trial/#class-definition","title":"Class Definition","text":"<pre><code>class Trial(self, trial_id: &lt;class 'str'&gt;, parameters: Dict[str, Any]):\n    \"\"\"\n    Initialize a new trial.\n    **Args:**\n    * **trial_id**: Unique identifier for the trial\n    * **parameters**: Dictionary of parameters for this trial\n    \"\"\"\n</code></pre>"},{"location":"api/trial/#methods","title":"Methods","text":"Method Description <code>add_job</code> Add a job to this trial. <code>check_status</code> Check the status of all jobs and update the trial state. <code>get_results</code> Gather results from all jobs. <code>run</code> Run all jobs in this trial."},{"location":"api/trial/#method-details","title":"Method Details","text":""},{"location":"api/trial/#add_job","title":"add_job","text":"<pre><code>def add_job(self, job: &lt;class 'Job'&gt;) -&gt; None\n</code></pre> <p>Add a job to this trial. Args: * job: The job to add</p>"},{"location":"api/trial/#check_status","title":"check_status","text":"<pre><code>def check_status(self) -&gt; &lt;enum 'TrialState'&gt;\n</code></pre> <p>Check the status of all jobs and update the trial state. Returns:   The current state of the trial</p>"},{"location":"api/trial/#get_results","title":"get_results","text":"<pre><code>def get_results(self) -&gt; Dict[str, Any]\n</code></pre> <p>Gather results from all jobs. Returns:   Dictionary of results</p>"},{"location":"api/trial/#run","title":"run","text":"<pre><code>def run(self) -&gt; None\n</code></pre> <p>Run all jobs in this trial.</p>"},{"location":"assets/","title":"Please add the AID2E logo image here","text":"<p>To use the logo image, you need to:</p> <ol> <li>Save the image you shared to this directory as <code>logo.png</code></li> <li>Create a smaller version for the favicon as <code>favicon.png</code> (ideally 32x32px or 64x64px)</li> </ol> <p>You can do this by downloading the image you shared and copying it to this directory manually.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section provides step-by-step tutorials for common use cases of the Scheduler library.</p>"},{"location":"tutorials/#basic-tutorials","title":"Basic Tutorials","text":"<ul> <li>Basic Detector Optimization: Learn how to optimize detector parameters using a simple objective function</li> <li>Script-Based Optimization: Use external scripts for your optimization objective</li> <li>Container-Based Optimization: Run containerized applications as optimization objectives</li> </ul>"},{"location":"tutorials/#advanced-tutorials","title":"Advanced Tutorials","text":"<ul> <li>Using Slurm for Execution: Scale up your optimization with Slurm cluster computing</li> <li>Using PanDA for Execution: Distribute optimization trials across the grid with PanDA</li> <li>Batch Trial Submission: Submit multiple trials in parallel for efficient exploration</li> <li>Asynchronous Execution: Run trials asynchronously for better resource utilization</li> <li>Experiment Persistence: Save and load experiments to resume optimization</li> </ul>"},{"location":"tutorials/asynchronous_execution/","title":"Asynchronous Execution","text":"<p>Work in Progress</p> <p>This documentation page is currently under development. Check back soon for complete information.</p> <p>This tutorial explains how to use asynchronous execution with the Scheduler for AID2E.</p>"},{"location":"tutorials/asynchronous_execution/#overview","title":"Overview","text":"<p>Asynchronous execution allows you to run multiple optimization trials in parallel without blocking, which can significantly speed up the optimization process.</p>"},{"location":"tutorials/asynchronous_execution/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of Ax optimization</li> <li>A working installation of Scheduler for AID2E</li> </ul>"},{"location":"tutorials/asynchronous_execution/#implementation","title":"Implementation","text":"<p>More detailed documentation coming soon.</p>"},{"location":"tutorials/asynchronous_execution/#examples","title":"Examples","text":"<p>Code examples will be provided in future updates.</p>"},{"location":"tutorials/batch_trial_submission/","title":"Tutorial: Batch Trial Submission","text":"<p>This tutorial demonstrates how to use the batch trial submission feature of the Scheduler library. Batch trials allow you to submit multiple trials at once, which can be more efficient than submitting them one by one.</p>"},{"location":"tutorials/batch_trial_submission/#prerequisites","title":"Prerequisites","text":"<ul> <li>Scheduler library installed</li> <li>Basic understanding of the Scheduler and Ax</li> </ul>"},{"location":"tutorials/batch_trial_submission/#step-1-import-required-libraries","title":"Step 1: Import Required Libraries","text":"<pre><code>from ax.service.ax_client import AxClient\nfrom scheduler import AxScheduler, JobLibRunner\n</code></pre>"},{"location":"tutorials/batch_trial_submission/#step-2-define-your-objective-function","title":"Step 2: Define Your Objective Function","text":"<pre><code>def objective_function(parameterization):\n    \"\"\"Simple objective function for demonstration.\"\"\"\n    x = parameterization[\"x\"]\n    y = parameterization[\"y\"]\n\n    # Simple objective function: Rosenbrock function\n    a = 1\n    b = 100\n    objective = (a - x)**2 + b * (y - x**2)**2\n\n    return {\"objective\": objective}\n</code></pre>"},{"location":"tutorials/batch_trial_submission/#step-3-initialize-ax-client-and-define-parameter-space","title":"Step 3: Initialize Ax Client and Define Parameter Space","text":"<pre><code># Initialize Ax client\nax_client = AxClient()\n\n# Define the parameter space\nax_client.create_experiment(\n    name=\"batch_trial_demo\",\n    parameters=[\n        {\n            \"name\": \"x\",\n            \"type\": \"range\",\n            \"bounds\": [-2.0, 2.0],\n            \"value_type\": \"float\",\n        },\n        {\n            \"name\": \"y\",\n            \"type\": \"range\",\n            \"bounds\": [-2.0, 2.0],\n            \"value_type\": \"float\",\n        },\n    ],\n    objectives={\"objective\": \"minimize\"},\n)\n</code></pre>"},{"location":"tutorials/batch_trial_submission/#step-4-create-a-runner-and-scheduler","title":"Step 4: Create a Runner and Scheduler","text":"<pre><code># Create a runner for local execution\nrunner = JobLibRunner(n_jobs=4)  # Use 4 cores\n\n# Create the scheduler\nscheduler = AxScheduler(ax_client, runner)\n\n# Set the objective function\nscheduler.set_objective_function(objective_function)\n</code></pre>"},{"location":"tutorials/batch_trial_submission/#step-5-use-batch-trial-context","title":"Step 5: Use Batch Trial Context","text":"<pre><code># Create a batch of trials\nwith scheduler.batch_trial_context() as batch:\n    # Add trials with specific parameter values\n    batch.add_trial({\"x\": 0.5, \"y\": 0.5})\n    batch.add_trial({\"x\": -0.5, \"y\": 0.5})\n    batch.add_trial({\"x\": 0.5, \"y\": -0.5})\n    batch.add_trial({\"x\": -0.5, \"y\": -0.5})\n\n    # The trials will be run when exiting the context\n</code></pre>"},{"location":"tutorials/batch_trial_submission/#step-6-run-additional-optimization","title":"Step 6: Run Additional Optimization","text":"<pre><code># Run more trials using standard optimization\nbest_params = scheduler.run_optimization(max_trials=10)\n\n# Print the results\nprint(\"Best parameters:\")\nprint(best_params)\n</code></pre>"},{"location":"tutorials/batch_trial_submission/#complete-example","title":"Complete Example","text":"<p>Here's the complete example:</p> <pre><code>from ax.service.ax_client import AxClient\nfrom scheduler import AxScheduler, JobLibRunner\n\ndef objective_function(parameterization):\n    \"\"\"Simple objective function for demonstration.\"\"\"\n    x = parameterization[\"x\"]\n    y = parameterization[\"y\"]\n\n    # Simple objective function: Rosenbrock function\n    a = 1\n    b = 100\n    objective = (a - x)**2 + b * (y - x**2)**2\n\n    return {\"objective\": objective}\n\ndef main():\n    # Initialize Ax client\n    ax_client = AxClient()\n\n    # Define the parameter space\n    ax_client.create_experiment(\n        name=\"batch_trial_demo\",\n        parameters=[\n            {\n                \"name\": \"x\",\n                \"type\": \"range\",\n                \"bounds\": [-2.0, 2.0],\n                \"value_type\": \"float\",\n            },\n            {\n                \"name\": \"y\",\n                \"type\": \"range\",\n                \"bounds\": [-2.0, 2.0],\n                \"value_type\": \"float\",\n            },\n        ],\n        objectives={\"objective\": \"minimize\"},\n    )\n\n    # Create a runner for local execution\n    runner = JobLibRunner(n_jobs=4)  # Use 4 cores\n\n    # Create the scheduler\n    scheduler = AxScheduler(ax_client, runner)\n\n    # Set the objective function\n    scheduler.set_objective_function(objective_function)\n\n    # Create a batch of trials with manually specified values\n    print(\"Running batch of manually specified trials...\")\n    with scheduler.batch_trial_context() as batch:\n        # Add trials with specific parameter values\n        batch.add_trial({\"x\": 0.5, \"y\": 0.5})\n        batch.add_trial({\"x\": -0.5, \"y\": 0.5})\n        batch.add_trial({\"x\": 0.5, \"y\": -0.5})\n        batch.add_trial({\"x\": -0.5, \"y\": -0.5})\n\n    # Print results of the batch trials\n    print(\"\\nResults from batch trials:\")\n    for trial_idx in range(ax_client.experiment.num_trials):\n        trial = ax_client.experiment.trials[trial_idx]\n        params = trial.arm.parameters\n        metrics = trial.objective_mean\n        print(f\"Trial {trial_idx}: Params {params}, Objective: {metrics}\")\n\n    # Run more trials using standard optimization\n    print(\"\\nRunning additional optimization trials...\")\n    best_params = scheduler.run_optimization(max_trials=6)  # 6 more trials\n\n    # Print the final results\n    print(\"\\nBest parameters after all trials:\")\n    print(best_params)\n\n    # Save the experiment for later analysis\n    scheduler.save_experiment(\"batch_trial_results.json\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/batch_trial_submission/#advanced-batch-trials-with-ax-generation-strategy","title":"Advanced: Batch Trials with Ax Generation Strategy","text":"<p>You can also combine batch trials with Ax's generation strategy to generate multiple parameter sets at once:</p> <pre><code># Create a batch of model-generated trials\nprint(\"Running batch of model-generated trials...\")\nwith scheduler.batch_trial_context() as batch:\n    # Generate multiple parameter sets using the model\n    for _ in range(4):\n        parameters, trial_index = ax_client.get_next_trial()\n        # Add the trial to the batch\n        batch.add_trial(parameters)\n</code></pre>"},{"location":"tutorials/batch_trial_submission/#advanced-asynchronous-batch-execution","title":"Advanced: Asynchronous Batch Execution","text":"<p>For more efficient resource utilization, you can run batch trials asynchronously:</p> <pre><code># Create the scheduler with asynchronous execution\nscheduler = AxScheduler(\n    ax_client, \n    runner,\n    config={\n        'synchronous': False,  # Run trials asynchronously\n        'monitoring_interval': 1  # Check status every second\n    }\n)\n\n# Set the objective function\nscheduler.set_objective_function(objective_function)\n\n# Create an asynchronous batch of trials\nwith scheduler.batch_trial_context() as batch:\n    for _ in range(10):\n        parameters, trial_index = ax_client.get_next_trial()\n        batch.add_trial(parameters)\n\n# Wait for all trials to complete\nscheduler.wait_for_completed_trials()\n</code></pre>"},{"location":"tutorials/batch_trial_submission/#next-steps","title":"Next Steps","text":"<ul> <li>Combine batch trials with Slurm Execution for high-performance computing</li> <li>Use batch trials with Container-Based Optimization for reproducible experiments</li> <li>Learn about saving and loading experiments in the Experiment Persistence tutorial</li> </ul>"},{"location":"tutorials/container_based_optimization/","title":"Tutorial: Container-Based Optimization","text":"<p>This tutorial demonstrates how to use the Scheduler library with containers (Docker or Singularity) for optimization tasks. Containers provide consistent execution environments and can help ensure reproducibility of results.</p>"},{"location":"tutorials/container_based_optimization/#prerequisites","title":"Prerequisites","text":"<ul> <li>Scheduler library installed</li> <li>Docker or Singularity installed on your system</li> <li>Basic understanding of containers</li> </ul>"},{"location":"tutorials/container_based_optimization/#step-1-prepare-your-container","title":"Step 1: Prepare Your Container","text":"<p>First, you need a container image that includes your simulation or analysis code. Here's an example Dockerfile:</p> <pre><code># Use a suitable base image\nFROM python:3.9-slim\n\n# Install dependencies\nRUN pip install numpy scipy pandas matplotlib\n\n# Copy your simulation code\nCOPY simulation.py /app/simulation.py\n\n# Set the working directory\nWORKDIR /app\n\n# The container will be run with the command passed via the scheduler\n</code></pre> <p>Build the container image:</p> <pre><code>docker build -t epic-simulation:latest .\n</code></pre>"},{"location":"tutorials/container_based_optimization/#step-2-import-required-libraries","title":"Step 2: Import Required Libraries","text":"<pre><code>from ax.service.ax_client import AxClient\nfrom scheduler import AxScheduler, JobLibRunner\n</code></pre>"},{"location":"tutorials/container_based_optimization/#step-3-initialize-ax-client-and-define-parameter-space","title":"Step 3: Initialize Ax Client and Define Parameter Space","text":"<pre><code># Initialize Ax client\nax_client = AxClient()\n\n# Define the parameter space\nax_client.create_experiment(\n    name=\"container_detector_optimization\",\n    parameters=[\n        {\n            \"name\": \"field_strength\",\n            \"type\": \"range\",\n            \"bounds\": [1.0, 3.0],\n            \"value_type\": \"float\",\n        },\n        {\n            \"name\": \"detector_length\",\n            \"type\": \"range\",\n            \"bounds\": [4.0, 8.0],\n            \"value_type\": \"float\",\n        },\n        {\n            \"name\": \"detector_radius\",\n            \"type\": \"range\",\n            \"bounds\": [1.0, 3.0],\n            \"value_type\": \"float\",\n        },\n    ],\n    objectives={\n        \"resolution\": \"minimize\",\n        \"acceptance\": \"maximize\",\n    },\n    outcome_constraints=[\"cost &lt;= 100.0\"],\n)\n</code></pre>"},{"location":"tutorials/container_based_optimization/#step-4-create-a-runner-with-container-support","title":"Step 4: Create a Runner with Container Support","text":"<pre><code># Create a runner with container support\nrunner = JobLibRunner(\n    n_jobs=4,  # Use 4 cores\n    config={\n        'container_engine': 'docker',  # or 'singularity'\n        'tmp_dir': './container_tmp'   # Directory for temporary files\n    }\n)\n</code></pre>"},{"location":"tutorials/container_based_optimization/#step-5-create-the-scheduler-and-set-container-objective","title":"Step 5: Create the Scheduler and Set Container Objective","text":"<pre><code># Create the scheduler\nscheduler = AxScheduler(\n    ax_client, \n    runner,\n    config={\n        'job_output_dir': './container_outputs',  # Directory for job outputs\n        'monitoring_interval': 10                 # Check status every 10 seconds\n    }\n)\n\n# Set the container objective\nscheduler.set_container_objective(\n    container_image=\"epic-simulation:latest\",\n    container_command=\"python simulation.py {params_file} {output_file}\",\n    container_options={\n        'volumes': {\n            './data': '/app/data',  # Mount local data directory\n        },\n        'env_vars': {\n            'DEBUG': '1'\n        }\n    }\n)\n</code></pre> <p>The <code>{params_file}</code> and <code>{output_file}</code> placeholders will be automatically replaced with the paths to the parameter and output files.</p>"},{"location":"tutorials/container_based_optimization/#step-6-run-the-optimization","title":"Step 6: Run the Optimization","text":"<pre><code># Run the optimization\nbest_params = scheduler.run_optimization(max_trials=20)\n\n# Print the results\nprint(\"Best parameters:\")\nprint(best_params)\n\n# Get the best metrics\nbest_metrics = ax_client.get_best_trial().values\nprint(\"Best metrics:\")\nprint(best_metrics)\n</code></pre>"},{"location":"tutorials/container_based_optimization/#complete-example","title":"Complete Example","text":"<p>Writing the complete example now.</p>"},{"location":"tutorials/container_based_optimization/#next-steps","title":"Next Steps","text":"<ul> <li>Try combining containers with Slurm execution for scalable, reproducible optimization</li> <li>Explore saving and loading container-based experiments in the Experiment Persistence tutorial</li> <li>Learn about batch trial submission in the Batch Trial Submission tutorial</li> </ul>"},{"location":"tutorials/container_jobs/","title":"Container Jobs","text":"<p>Work in Progress</p> <p>This documentation page is currently under development. Check back soon for complete information.</p> <p>This tutorial explains how to use container jobs with the Scheduler for AID2E.</p>"},{"location":"tutorials/container_jobs/#overview","title":"Overview","text":"<p>Container jobs allow you to package your optimization workloads in containers (Docker, Singularity, etc.) for better portability and reproducibility.</p>"},{"location":"tutorials/container_jobs/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of container technologies (Docker, Singularity)</li> <li>A working installation of Scheduler for AID2E</li> </ul>"},{"location":"tutorials/container_jobs/#implementation","title":"Implementation","text":"<p>More detailed documentation coming soon.</p>"},{"location":"tutorials/container_jobs/#examples","title":"Examples","text":"<p>Code examples will be provided in future updates.</p>"},{"location":"tutorials/detector_optimization/","title":"Tutorial: Basic Detector Optimization","text":"<p>This tutorial shows how to use the Scheduler library for optimizing detector parameters. We'll create a simple objective function that evaluates detector performance based on field strength, detector length, and detector radius.</p>"},{"location":"tutorials/detector_optimization/#prerequisites","title":"Prerequisites","text":"<ul> <li>Scheduler library installed (see Installation)</li> <li>Basic understanding of Bayesian optimization with Ax</li> <li><code>eic-shell</code> container and a own version of <code>epic</code> detector geometry </li> </ul>"},{"location":"tutorials/detector_optimization/#step-1-import-required-libraries","title":"Step 1: Import Required Libraries","text":"<pre><code>import numpy as np\nfrom ax.service.ax_client import AxClient\nfrom scheduler import AxScheduler, JobLibRunner\n</code></pre>"},{"location":"tutorials/detector_optimization/#step-2-define-your-objective-function","title":"Step 2: Define Your Objective Function","text":"<p>Write the holistic example here</p>"},{"location":"tutorials/detector_optimization/#next-steps","title":"Next Steps","text":"<ul> <li>Try modifying the objective function to include more realistic detector physics</li> <li>Experiment with different parameter ranges and constraints</li> <li>Check out the Slurm Execution Tutorial to scale up your optimization</li> </ul>"},{"location":"tutorials/experiment_persistence/","title":"Experiment Persistence","text":"<p>This documentation is currently under development.</p> <p>Experiment persistence allows you to save the state of your optimization experiments and resume them later. This is especially useful for long-running experiments or when you need to pause and resume optimization.</p>"},{"location":"tutorials/experiment_persistence/#features","title":"Features","text":"<ul> <li>Save experiment state to disk</li> <li>Resume optimization from a saved state</li> <li>Track optimization history across sessions</li> </ul> <p>Check back soon for the complete documentation.</p>"},{"location":"tutorials/panda_execution/","title":"Tutorial: Using PanDA Execution","text":"<p>The PanDA (Production and Distributed Analysis) system is a high-performance workload management system originally developed by the ATLAS experiment at CERN. It is now widely used for managing distributed computing workloads across Grid, Cloud, and HPC environments. The iDDS (intelligent Data Delivery Service) is a powerful and flexible workflow system to orchestrate multi-step, data-aware, and ML-friendly workflows over distributed computing systems.</p> <p>This tutorial explains how to use PANDA execution with the Scheduler for AID2E.</p>"},{"location":"tutorials/panda_execution/#prerequisites","title":"Prerequisites","text":"<ul> <li>User registeration based on CILogon https://panda-iam-doma.cern.ch/</li> <li>PanDA documents https://panda-wms.readthedocs.io/en/latest/</li> <li>iDDS documents https://idds.readthedocs.io/en/latest/</li> </ul>"},{"location":"tutorials/panda_execution/#initialize-the-environment","title":"Initialize the environment","text":"<pre><code>python3.12 -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip black ruff\npip install idds-client idds-common idds-workflow panda-client\n</code></pre>"},{"location":"tutorials/panda_execution/#setup-the-environment","title":"Setup the environment","text":"<ul> <li>Setup virturl environment</li> </ul> <pre><code>source ./setup.sh\n</code></pre> <p>setup.sh [<code>https://github.com/aid2e/scheduler_epic/blob/pandaidds/setup.sh</code>]</p> <ul> <li>Setup PanDA(BNL) environment (This PanDA service is maintained by BNL to run jobs at WLCG, OSG, HPC and so on, with Rucio integrated). <pre><code>source setup_panda_bnl.sh\n</code></pre> <code>setup_panda_bnl.sh</code> [<code>https://github.com/aid2e/scheduler_epic/blob/pandaidds/setup_panda_bnl.sh</code>]</li> </ul>"},{"location":"tutorials/panda_execution/#simple-example","title":"Simple example","text":""},{"location":"tutorials/panda_execution/#setup-1-import-python-libraries","title":"Setup 1: import python libraries","text":"<pre><code>import logging\n\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\nfrom scheduler import AxScheduler, PanDAiDDSRunner\nfrom scheduler.utils.common import setup_logging\n</code></pre>"},{"location":"tutorials/panda_execution/#setup-2-define-the-objectvie-function","title":"Setup 2: Define the objectvie function","text":"<pre><code>def objective_function(x, y):\n    return {\"objective\": (x - 0.5) ** 2 + (y - 0.5) ** 2}\n</code></pre>"},{"location":"tutorials/panda_execution/#setup-3-initialize-the-ax-client-and-create-the-experiment","title":"Setup 3: Initialize the Ax client and create the experiment","text":"<p>With python libraries like <code>cloudpickle</code> and <code>dill</code>, users can ship the function codes to remote computing resources to execute them. However, in complex use cases, one function may need to call a lot of other functions and libraries developed. Many of them are locally developed. Instead of only shipping the codes of one function to remote computing resources. The idea of PanDA-iDDS is to ship all codes in the current working directory and then try to import them at remote computing resources.</p> <p>NOTE: The python codes will be imported at remote computing resources. To avoid executing some codes during importing, You need to put those codes in <code>if __name__ == \"__main__\":</code></p> <pre><code>if __name__ == \"__main__\":\n    setup_logging(log_level=\"debug\")\n\n    logging.debug(\"setup ax client\")\n    # Initialize Ax client\n    ax_client = AxClient()\n\n    logging.info(\"Creating experiment\")\n\n    # Define your parameter space\n    ax_client.create_experiment(\n        name=\"my_experiment\",\n        parameters=[\n            {\n                \"name\": \"x\",\n                \"type\": \"range\",\n                \"bounds\": [0.0, 1.0],\n                \"value_type\": \"float\",\n            },\n            {\n                \"name\": \"y\",\n                \"type\": \"range\",\n                \"bounds\": [0.0, 1.0],\n                \"value_type\": \"float\",\n            },\n        ],\n        objectives={\"objective\": ObjectiveProperties(minimize=True)},\n    )\n</code></pre>"},{"location":"tutorials/panda_execution/#step-4-initializa-the-panda-runner","title":"Step 4: Initializa the PanDA runner","text":"<pre><code>    # Virtual env deployed on cvmfs\n    # Libaries like Ax are required during importing at remote computing resources.\n    init_env = [\n        \"source /cvmfs/unpacked.cern.ch/registry.hub.docker.com/fyingtsai/eic_xl:24.11.1/opt/conda/setup_mamba.sh;\"\n        \"source /cvmfs/unpacked.cern.ch/registry.hub.docker.com/fyingtsai/eic_xl:24.11.1/opt/conda/dRICH-MOBO//MOBO-tools/setup_new.sh;\"\n        \"command -v singularity &amp;&gt; /dev/null || export SINGULARITY=/cvmfs/oasis.opensciencegrid.org/mis/singularity/current/bin/singularity;\"\n        \"export AIDE_HOME=$(pwd);\"\n        \"export PWD_PATH=$(pwd);\"\n        'export SINGULARITY_OPTIONS=\"--bind /cvmfs:/cvmfs,$(pwd):$(pwd)\"; '\n        \"export SIF=/cvmfs/singularity.opensciencegrid.org/eicweb/eic_xl:24.11.1-stable; export SINGULARITY_BINDPATH=/cvmfs,/afs; \"\n        \"env; \"\n    ]\n    init_env = \" \".join(init_env)\n\n    panda_attrs = {\n        \"name\": \"user.wguan.my_experiment\",\n        \"init_env\": init_env,\n        \"cloud\": \"US\",\n        \"queue\": \"BNL_PanDA_1\",  # BNL_OSG_PanDA_1, BNL_PanDA_1\n        \"source_dir\": None,  # used to upload files in the source directory to PanDA, which will be used for the remote jobs.\n                             # None is the current directory.\n        \"source_dir_parent_level\": 1,\n        \"exclude_source_files\": [\n            r\"(^|/)\\.[^/]+\",    # file starts with \".\"\n            \"doc*\", \"DTLZ2*\", \".*json\", \".*log\", \"work\", \"log\", \"OUTDIR\",\n            \"calibrations\", \"fieldmaps\", \"gdml\", \"EICrecon-drich-mobo\",\n            \"eic-software\", \"epic-geom-drich-mobo\", \"irt\", \"share\", \"back*\",\n            \"__pycache__\"\n        ],\n        \"max_walltime\": 3600,\n        \"core_count\": 1,\n        \"total_memory\": 4000,\n        \"enable_separate_log\": True,\n        \"job_dir\": None,\n    }\n\n    # Create a runner\n    runner = PanDAiDDSRunner(**panda_attrs)\n    logging.info(f\"created runner: {runner}\")\n</code></pre>"},{"location":"tutorials/panda_execution/#step-5-create-a-scheduler-to-run-optimization","title":"Step 5: Create a scheduler to run optimization","text":"<pre><code>    # Create the scheduler\n    scheduler = AxScheduler(ax_client, runner)\n    logging.info(f\"created scheduler: {scheduler}\")\n\n    # Set the objective function\n    scheduler.set_objective_function(objective_function)\n\n    logging.info(\"running optimization\")\n    # Run the optimization\n    best_params = scheduler.run_optimization(max_trials=10)\n    print(\"Best parameters:\", best_params)\n</code></pre>"},{"location":"tutorials/panda_execution/#step-6-complete-example","title":"Step 6: Complete Example","text":"<p>The full example can be found in <code>test_pandaidds_simple.py</code> [<code>https://github.com/aid2e/scheduler_epic/blob/pandaidds/tests/pandaidds/test_pandaidds_simple.py</code>]</p>"},{"location":"tutorials/panda_execution/#multiple-step-example","title":"Multiple Step example","text":"<p>This example is similar to the example above. Here we only show the differences.</p>"},{"location":"tutorials/panda_execution/#define-multiple-step-functions","title":"Define multiple step functions","text":"<p>In this example, <code>x</code> and <code>y</code> and hyperparameters which will be filled by Ax. <code>xyz</code> is the results from the first function. The system will automatically collect the results from the first function and fill it to the second function for every set of hyperparameter <code>x</code> and <code>y</code>.</p> <pre><code># Define your objective function\ndef objective_function_step(x, y):\n    return {\"xyz\": (x - 0.5) ** 2 + (y - 0.5) ** 2}\n\n\ndef objective_function_ana(x, y, xyz):\n    return {\"objective\": (x - 0.5) ** 2 + (y - 0.5) ** 2 + xyz * 0.1}\n</code></pre>"},{"location":"tutorials/panda_execution/#define-the-objective-function-with-a-multistepfunction","title":"Define the objective function with a MultiStepFunction","text":"<p>In the MultiStepFunction, the first step runs with PanDAiDDSRunner and the second step runs at local JobLibRunner.</p> <pre><code>    objective_function = MultiStepsFunction(\n        objective_funcs={\n            \"simrecoana\": {\n                \"func\": objective_function_step,\n                \"job_type\": JobType.FUNCTION,\n                \"runner\": PanDAiDDSRunner(**panda_attrs)\n            },\n            \"final\": {\n                \"func\": objective_function_ana,\n                \"job_type\": JobType.FUNCTION,\n                \"runner\": JobLibRunner(n_jobs=-1),\n                \"parent_result_parameter_name\": \"xyz\",\n            },\n        },\n        deps={\n            \"final\": {\"parent\": \"simrecoana\", \"dep_type\": \"results\", \"dep_map\": \"one2one\"},\n        },\n        final=\"final\",    # if final is not set, it will use the last step in objective_funcs.\n                          # The final step will set its result as the job's result\n    )\n</code></pre>"},{"location":"tutorials/panda_execution/#full-multistepfunction-example","title":"Full MultiStepFunction example","text":"<p>The full example can be found in <code>test_pandaidds_multi_steps_2.py</code> [<code>https://github.com/aid2e/scheduler_epic/blob/pandaidds/tests/pandaidds/test_pandaidds_multi_steps_2.py</code>]</p>"},{"location":"tutorials/panda_execution/#multiple-step-with-global-parameters","title":"Multiple Step with global parameters","text":"<p>For a set of hyperparameters, one may need to evaluate it with different conditions. For example, for detector design, with on detecotr setting, we may need to evaluate the detector performance against different types of particles. Global parameters are designed for it.</p>"},{"location":"tutorials/panda_execution/#global-parameters","title":"Global parameters","text":"<p><pre><code>global_parameters = {\n    \"particles\": [\"pi+\", \"kaon+\"],\n    \"eta_points\": [0.1, 0.2]\n}\n</code></pre> In this example, it will generate a lit of parameters. For every set of hyperparameters, it will execute the objective function 4 times with one column in the list of the parameters below. <pre><code> [{'eta_points': 0.1, 'particles': 'pi+'},\n  {'eta_points': 0.1, 'particles': 'kaon+'},\n  {'eta_points': 0.2, 'particles': 'pi+'},\n  {'eta_points': 0.2, 'particles': 'kaon+'}]\n</code></pre></p>"},{"location":"tutorials/panda_execution/#define-objective-function","title":"Define objective function","text":"<p>In the objective function, <code>x</code> and <code>y</code> are hyperparameters. <code>particles</code> and <code>eta_points</code> are global parameters.</p> <pre><code>def objective_function_step(x, y, particles, eta_points):\n    if particles == \"pi+\":\n        return {\"xyz\": ((x - 0.5) ** 3 + (y - 0.5) ** 3) * eta_points}\n    elif particles == \"kaon+\":\n        return {\"xyz\": ((x - 0.5) ** 2 + (y - 0.5) ** 2) * eta_points}\n    else:\n        return {\"xyz\": 0.1}\n</code></pre>"},{"location":"tutorials/panda_execution/#merge-objectives","title":"Merge objectives","text":"<p>In this example, the fist step will return <code>xyz</code>. However, with different global parameters, it will return a different result. If we define the dependency map previous step and the merge step is <code>all2one</code>. The return value {<code>xzz</code>: } will be mapped to: <pre><code>{\n    `xyz`: {\n        (('eta_points': 0.1), ('particles': 'pi+')): &lt;value1&gt;,\n        (('eta_points': 0.1), ('particles': 'kaon+')): &lt;value2&gt;,\n        (('eta_points': 0.2), ('particles': 'pi+')): &lt;value3&gt;,\n        (('eta_points': 0.2), ('particles': 'kaon+')): &lt;value4&gt;\n    }\n}\n</code></pre> <p>Here is an example of the merge function. <pre><code>def objective_function_ana(x, y, xyz):\n    # print(f\"global_parameters: {global_parameters}\")\n    sorted_keys = sorted(global_parameters.keys())\n    combinations = [dict(zip(sorted_keys, values)) for values in product(*[global_parameters[k] for k in sorted_keys])]\n    job_keys = combinations\n    # print(f\"job_keys: {job_keys}\")\n    # print(f\"xyz: {xyz}\")\n    xyz_sum = sum([xyz[tuple(k.items())] for k in job_keys])\n    return {\"objective\": (x - 0.5) ** 2 + (y - 0.5) ** 2 + xyz_sum * 0.1}\n</code></pre></p>"},{"location":"tutorials/panda_execution/#define-the-objective-function-of-multistepfunction-with-global-parameters","title":"Define the objective function of MultiStepFunction with global parameters","text":"<pre><code>    objective_function = MultiStepsFunction(\n        objective_funcs={\n            \"simrecoana\": {\n                \"func\": objective_function_step,\n                \"job_type\": JobType.FUNCTION,\n                \"runner\": PanDAiDDSRunner(**panda_attrs)\n            },\n            \"final\": {\n                \"func\": objective_function_ana,\n                \"job_type\": JobType.FUNCTION,\n                \"runner\": JobLibRunner(n_jobs=-1),\n                \"parent_result_parameter_name\": \"xyz\",\n            },\n        },\n        deps={\n            \"final\": {\"parent\": \"simrecoana\", \"dep_type\": \"results\", \"dep_map\": \"all2one\"},\n        },\n        global_parameters=global_parameters,\n        global_parameters_steps=[\"simrecoana\"],\n        final=\"final\",                # if final is not set, it will use the last step in objective_funcs.\n                                      # The final step will set its result as the job's result\n    )\n</code></pre>"},{"location":"tutorials/panda_execution/#full-multistepfunction-example_1","title":"Full MultiStepFunction example","text":"<p>The full example can be found in <code>test_pandaidds_multi_steps_3.py</code> [<code>https://github.com/aid2e/scheduler_epic/blob/pandaidds/tests/pandaidds/test_pandaidds_multi_steps_3.py</code>]</p>"},{"location":"tutorials/panda_execution/#multiple-step-with-rucio-supports","title":"Multiple Step with Rucio supports","text":"<p>Rucio is a large-scale, distributed data management system. Here we will use Rucio to manage input/output datasets.</p>"},{"location":"tutorials/panda_execution/#define-the-objective-functions","title":"Define the objective functions","text":"<p>In the <code>objective_function_step_simreco</code> function, we write the results to a file. We will upload this file to Rucio. (In real use cases, this step should generate root files which are big and the content of the root files are difficult to be transferred through function return values. So we need to store the root files in some storages with Rucio.)</p> <p>In the <code>objective_function_step_ana</code> function, we have a new parameter <code>input_file_names</code> as a placeholder for the list of file names from a dataset (we will describe how to tell PanDA to fill this parameter later).</p> <pre><code>def objective_function_step_simreco(x, y, particles, eta_points):\n    if particles == \"pi+\":\n        ret = {\"xyz\": ((x - 0.5) ** 3 + (y - 0.5) ** 3) * eta_points}\n    elif particles == \"kaon+\":\n        ret = {\"xyz\": ((x - 0.5) ** 2 + (y - 0.5) ** 2) * eta_points}\n    else:\n        ret = {\"xyz\": 0.1}\n\n    with open(\"my_test.txt\", \"w\") as f:\n        json.dump(ret, f)\n\n\ndef objective_function_step_ana(x, y, particles, eta_points, input_file_names):\n    ret = {}\n\n    for input_file_name in input_file_names:\n        try:\n            with open(input_file_name, \"r\") as f:\n                data = json.load(f)\n                for k, v in data.items():\n                    # Assumes v is numeric; otherwise raises an error\n                    ret[k] = ret.get(k, 0) + v\n        except Exception as e:\n            print(f\"Error reading file {input_file_name}: {e}\")\n\n    return ret\n</code></pre>"},{"location":"tutorials/panda_execution/#define-the-objective-function-of-multistepfunction-with-datasets","title":"Define the objective function of MultiStepFunction with datasets","text":"<p>In this example, for the <code>simreco</code> step, we set <code>with_output_dataset</code> and define the <code>output_file</code> and <code>output_dataset</code>. The output file must be produced by <code>objective_function_step_simreco</code>. Otherwise PanDA will report errors that <code>output_file</code> cannot be found. Here we defined total events <code>num_events</code>=200 and <code>num_events_per_job</code>=100. So PanDA will generate two jobs. Every job will produce one <code>output_file</code> with name <code>my_test.txt</code>. To avoid overwriting each other, before uploading the <code>output_file</code> to Rucio, PanDA will rename the output with <code>dataset_name</code>+<code>additional sequence name</code>+<code>output_file name</code>.</p> <p>In the <code>ana</code> (analysis) step, we set <code>with_input_datasets</code> and define <code>\"input_datasets\": {\"input_file_names\": f\"{dataset_name_prefix}.simreco.#global_parameter_key.#job_id\"}</code>. Here the dataset name should be the same dataset name of the previous step. PanDA-iDDS will get the list of files in the input dataset and create an additional argument <code>\"input_file_names=&lt;file_list_in_dataset&gt;\"</code>. So the function <code>objective_function_step_ana</code> must have a placeholder argument for <code>'input_file_names'</code>. You can use any other names instead of <code>'input_file_names'</code>.</p> <p>In this example, we set <code>dep_type</code> to <code>datasets</code> and <code>return_func_results</code> to False. The scheduler will not wait for function results. PanDA will automatically trigger the next step when the datasets in <code>simreco</code> are done.</p> <pre><code>    objective_function = MultiStepsFunction(\n        objective_funcs={\n            \"simreco\": {\n                \"func\": objective_function_step_simreco,\n                \"job_type\": JobType.FUNCTION,\n                \"runner\": panda_idds_runner,\n                \"return_func_results\": False,    # here the outputs are in dataset, so no need to wait for function outputs\n                \"with_output_dataset\": True,\n                \"output_file\": \"my_test.txt\",\n                # if global parameters are used, please add '#global_parameter_key' to\n                # the dataset name. PanDA-iDDS will automatically replace it to different\n                # keys based on the global parameters. Otherwise, all files with different\n                # global parameters will be in the same dataset.\n                \"output_dataset\": f\"{dataset_name_prefix}.simreco.#global_parameter_key.#job_id\",\n                \"num_events\": 200,\n                \"num_events_per_job\": 100,\n            },\n            \"ana\": {\n                \"func\": objective_function_step_ana,\n                \"job_type\": JobType.FUNCTION,\n                \"runner\": panda_idds_runner,\n                \"with_input_datasets\": True,\n                # Here the dataset name should be the same dataset name of the previous step.\n                # PanDA-iDDS will get the list of files in the input dataset and create an\n                # additional argument \"input_file_names=&lt;file_list_in_dataset&gt;\".\n                # So the function objective_function_step_ana must have a placeholder argument\n                # for 'input_file_names'. You can use any other names instead of 'input_file_names'.\n                \"input_datasets\": {\"input_file_names\": f\"{dataset_name_prefix}.simreco.#global_parameter_key.#job_id\"},\n            },\n            \"final\": {\n                \"func\": objective_function_step_final,\n                \"job_type\": JobType.FUNCTION,\n                \"runner\": JobLibRunner(n_jobs=-1),\n                \"parent_result_parameter_name\": \"xyz\",      # will add a parameter xyz=&lt;get_parent_results&gt; to the func\n            },\n        },\n        deps={\n            \"final\": {\"parent\": \"ana\", \"dep_type\": \"results\", \"dep_map\": \"all2one\"},\n            \"ana\": {\"parent\": \"simreco\", \"dep_type\": \"datasets\", \"dep_map\": \"one2one\"},    # depends on the dataset. It will use rucio to manage the datasets.\n        },\n        global_parameters=global_parameters,\n        global_parameters_steps=[\"simreco\", \"ana\"],\n        final=\"final\",    # if final is not set, it will use the last step in objective_funcs.\n                          # The final step will set its result as the job's result\n    )\n</code></pre>"},{"location":"tutorials/panda_execution/#full-multistepfunction-with-rucio-example","title":"Full MultiStepFunction with Rucio example","text":"<p>The full example can be found in <code>test_pandaidds_multi_steps_4.py</code> [<code>https://github.com/aid2e/scheduler_epic/blob/pandaidds/tests/pandaidds/test_pandaidds_multi_steps_4.py</code>] and <code>test_pandaidds_multi_steps_5.py</code> [<code>https://github.com/aid2e/scheduler_epic/blob/pandaidds/tests/pandaidds/test_pandaidds_multi_steps_5.py</code>]. The difference between <code>test_pandaidds_multi_steps_4.py</code> and <code>test_pandaidds_multi_steps_5.py</code> is the <code>return_func_results</code>.</p>"},{"location":"tutorials/script_based_optimization/","title":"Script-Based Optimization","text":"<p>Work in Progress</p> <p>This documentation page is currently under development. Check back soon for complete information.</p> <p>This tutorial explains how to use script-based optimization with the Scheduler for AID2E.</p>"},{"location":"tutorials/script_based_optimization/#overview","title":"Overview","text":"<p>Script-based optimization allows you to define your optimization objectives using external scripts rather than Python functions.</p>"},{"location":"tutorials/script_based_optimization/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of Ax optimization</li> <li>A working installation of Scheduler for AID2E</li> </ul>"},{"location":"tutorials/script_based_optimization/#implementation","title":"Implementation","text":"<p>More detailed documentation coming soon.</p>"},{"location":"tutorials/script_based_optimization/#examples","title":"Examples","text":"<p>Code examples will be provided in future updates.</p>"},{"location":"tutorials/slurm_execution/","title":"Tutorial: Using Slurm for Execution","text":"<p>This tutorial demonstrates how to use the Scheduler library with Slurm for running optimization trials on a cluster. Using Slurm allows you to scale up your optimization by distributing trials across multiple compute nodes.</p>"},{"location":"tutorials/slurm_execution/#prerequisites","title":"Prerequisites","text":"<ul> <li>Scheduler library installed with Slurm support (<code>pip install -e .[slurm]</code>)</li> <li>Access to a Slurm cluster</li> <li>Basic understanding of Slurm concepts (partitions, job submission, etc.)</li> </ul>"},{"location":"tutorials/slurm_execution/#step-1-import-required-libraries","title":"Step 1: Import Required Libraries","text":"<pre><code>from ax.service.ax_client import AxClient\nfrom scheduler import AxScheduler, SlurmRunner\n</code></pre>"},{"location":"tutorials/slurm_execution/#step-2-define-your-objective-function","title":"Step 2: Define Your Objective Function","text":"<p>For this example, we'll use a script-based objective instead of a Python function, since this is more common in HPC environments:</p> <p>A toy function goes here</p>"},{"location":"tutorials/slurm_execution/#step-3-initialize-ax-client-and-define-parameter-space","title":"Step 3: Initialize Ax Client and Define Parameter Space","text":"<pre><code># Initialize Ax client\nax_client = AxClient()\n\n# Define the parameter space\nax_client.create_experiment(\n    name=\"slurm_optimization\",\n    parameters=[\n        {\n            \"name\": \"field_strength\",\n            \"type\": \"range\",\n            \"bounds\": [1.0, 3.0],\n            \"value_type\": \"float\",\n        },\n        {\n            \"name\": \"detector_length\",\n            \"type\": \"range\",\n            \"bounds\": [4.0, 8.0],\n            \"value_type\": \"float\",\n        },\n        {\n            \"name\": \"detector_radius\",\n            \"type\": \"range\",\n            \"bounds\": [1.0, 3.0],\n            \"value_type\": \"float\",\n        },\n    ],\n    objectives={\n        \"resolution\": \"minimize\",\n        \"acceptance\": \"maximize\",\n    },\n    outcome_constraints=[\"cost &lt;= 100.0\"],\n)\n</code></pre>"},{"location":"tutorials/slurm_execution/#step-4-create-a-slurm-runner","title":"Step 4: Create a Slurm Runner","text":"<pre><code># Create a runner for Slurm execution\nrunner = SlurmRunner(\n    partition=\"compute\",       # Specify your partition\n    time_limit=\"00:30:00\",     # 30 minutes per job\n    memory=\"4G\",               # 4GB of memory per job\n    cpus_per_task=4,           # 4 CPUs per task\n    config={\n        'modules': ['python/3.9'],  # Modules to load\n        'sbatch_options': {\n            'account': 'eic-project',       # Your account/allocation\n            'mail-user': 'user@example.com',  # Email for notifications\n            'mail-type': 'END,FAIL'           # When to send notifications\n        }\n    }\n)\n</code></pre>"},{"location":"tutorials/slurm_execution/#step-5-create-the-scheduler-and-set-script-objective","title":"Step 5: Create the Scheduler and Set Script Objective","text":"<pre><code># Create the scheduler\nscheduler = AxScheduler(\n    ax_client, \n    runner,\n    config={\n        'job_output_dir': './slurm_outputs',  # Directory for job outputs\n        'synchronous': False,                  # Run trials asynchronously\n        'monitoring_interval': 30              # Check status every 30 seconds\n    }\n)\n\n# Set the script objective\nscheduler.set_script_objective(\n    script_path=\"./simulation_script.py\",\n    script_options={\n        'interpreter': 'python',  # Use Python to run the script\n        'timeout': 1200           # Timeout in seconds (20 minutes)\n    }\n)\n</code></pre>"},{"location":"tutorials/slurm_execution/#step-6-run-the-optimization","title":"Step 6: Run the Optimization","text":"<pre><code># Run the optimization\nbest_params = scheduler.run_optimization(max_trials=20)\n\n# Print the results\nprint(\"Best parameters:\")\nprint(best_params)\n\n# Get the best metrics\nbest_metrics = ax_client.get_best_trial().values\nprint(\"Best metrics:\")\nprint(best_metrics)\n</code></pre>"},{"location":"tutorials/slurm_execution/#complete-example","title":"Complete Example","text":"<p>Write a complete example</p>"},{"location":"tutorials/slurm_execution/#next-steps","title":"Next Steps","text":"<ul> <li>Try using containers with Slurm by checking out the Container-Based Optimization tutorial</li> <li>Explore batch trial submission with Slurm in the Batch Trial Submission tutorial</li> <li>Learn how to save and load experiments in the Experiment Persistence tutorial</li> </ul>"}]}